#!/usr/bin/env python3
"""
Zone Optimization Script v7 - Complete Implementation with Historical Analysis
Integrates zone optimization analysis into your existing category analysis pipeline.

Enhanced for 6-week accelerated timeline using 2+ years of historical data.
Designed specifically for customer-week level data aggregation.
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
from collections import defaultdict, deque
import json
import warnings
from scipy import stats
import os
warnings.filterwarnings('ignore')
from openpyxl.formatting.rule import ColorScaleRule, CellIsRule, FormulaRule, DataBarRule
from openpyxl.utils import get_column_letter
from openpyxl.styles import DifferentialStyle, Font

# module-level config (already OK to have)
ALLOW_UPZONE_EPSILON = 0.01

# simple scope filter (CPA-only + exclude exceptions/discounts)
def _apply_scope_filters(df: pd.DataFrame, cpa_only=True, exclude_exceptions=True) -> pd.DataFrame:
    out = df.copy()
    if cpa_only and 'Price Source Type' in out.columns:
        out = out[out['Price Source Type'].astype(str).str.upper().eq('CPA')]
    if exclude_exceptions:
        for col in ['Exception Indicator','Discount Indicator']:
            if col in out.columns:
                out = out[~out[col].astype(str).str.upper().isin(['Y','1','TRUE'])]
    return out


def _col_index_by_header(ws, header_text: str) -> int:
    """Return 1-based column index for the first cell in row 1 that matches header_text."""
    for c in range(1, ws.max_column + 1):
        if (ws.cell(1, c).value or "").strip() == header_text:
            return c
    return -1

def _add_delta_column(ws, left_header: str, right_header: str, new_header: str) -> int:
    """
    Add a column computing right - left. Returns the new column index.
    """
    left_c = _col_index_by_header(ws, left_header)
    right_c = _col_index_by_header(ws, right_header)
    if left_c < 0 or right_c < 0:
        return -1
    new_c = ws.max_column + 1
    ws.cell(1, new_c, new_header)
    for r in range(2, ws.max_row + 1):
        ws.cell(r, new_c, f"={get_column_letter(right_c)}{r}-{get_column_letter(left_c)}{r}")
    return new_c


class ZoneOptimizationEngine:
    """
    Learns optimal zone suffixes for each Company + NPD Cuisine + Attribute Group combination
    to maximize volume and customer count growth, accepting margin sacrifice when overpriced.
    Enhanced for historical analysis with 2+ years of data for accelerated implementation.
    """
    
    def _winsorize(self, s: pd.Series, p_low=0.05, p_high=0.95):
        if s.empty:
            return s
        lo, hi = s.quantile(p_low), s.quantile(p_high)
        return s.clip(lower=lo, upper=hi)

    def _normalize_01(self, s: pd.Series):
        if s.empty:
            return s
        lo, hi = s.min(), s.max()
        if pd.isna(lo) or pd.isna(hi) or hi <= lo:
            return pd.Series(np.zeros(len(s)), index=s.index)
        return (s - lo) / (hi - lo)


    def _compute_behavior_signals(self, df: pd.DataFrame, window_weeks: int = None) -> pd.DataFrame:
        """
        Build behavior signals at (Company, Cuisine, AG, Price Source, Zone) level
        over a trailing window of fiscal weeks (default 12).
        Required columns in df (best case, customer-week rows):
        - 'Company Name', 'NPD Cuisine Type', 'Attribute Group ID', 'Price Source Type'
        - 'Zone_Suffix_Numeric' (we create it earlier if needed)
        - 'Company Customer Number'
        - 'Fiscal Week Number'
        - 'Pounds_CY' (numeric)
        - 'Computer Margin $ Per LB CY' (optional but useful)
        Fallback if you only have invoices: aggregate to customer-week first.
        """
        window_weeks = window_weeks or self.behavior_window_weeks

        # Guard: need at least week and customer id to compute repeat
        needed = {'Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type',
                'Zone_Suffix_Numeric','Company Customer Number','Fiscal Week Number'}
        if not needed.issubset(set(df.columns)):
            # graceful empty return -> no change to downstream logic
            return pd.DataFrame(columns=[
                'Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type','Zone_Suffix_Numeric',
                'active_customers','repeat_buyer_rate','active_buyer_share','avg_freq',
                'avg_recency','retention_4w','avg_margin_per_lb','lbs'
            ])

        # Keep CPA only if present in scope
        if 'Price Source Type' in df.columns:
            df = df[df['Price Source Type'].astype(str).str.upper().eq('CPA')]

        # Restrict to trailing window if full history exists (optional; many inputs are pre-windowed already)
        if 'Fiscal Week Number' in df.columns and df['Fiscal Week Number'].notna().any():
            max_wk = pd.to_numeric(df['Fiscal Week Number'], errors='coerce').max()
            df = df[pd.to_numeric(df['Fiscal Week Number'], errors='coerce') >= (max_wk - window_weeks + 1)]

        # If you have invoice-level rows (multiple per customer per week), roll to customer-week
        # Detect granularity by checking duplicates of (cust, week, combo+zone)
        keys_cw = ['Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type',
                'Zone_Suffix_Numeric','Company Customer Number','Fiscal Week Number']
        dup_mask = df.duplicated(keys_cw)
        if dup_mask.any():
            grp = df.groupby(keys_cw, dropna=False).agg(
                Pounds_CY=('Pounds_CY','sum') if 'Pounds_CY' in df.columns else ('Fiscal Week Number','size'),
                margin_per_lb=('Computer Margin $ Per LB CY','mean') if 'Computer Margin $ Per LB CY' in df.columns else ('Pounds_CY','size')
            ).reset_index()
        else:
            # Already at customer-week
            grp = df.copy()
            if 'Pounds_CY' not in grp.columns:
                grp['Pounds_CY'] = 0.0
            if 'Computer Margin $ Per LB CY' in grp.columns:
                grp['margin_per_lb'] = grp['Computer Margin $ Per LB CY']
            else:
                grp['margin_per_lb'] = np.nan

        # Per-customer window stats
        cust_keys = ['Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type',
                    'Zone_Suffix_Numeric','Company Customer Number']
        cw = grp  # customer-week
        by_cust = (cw.groupby(cust_keys, dropna=False)
                    .agg(weeks_active=('Fiscal Week Number','nunique'),
                        purchases_12w=('Fiscal Week Number','count'),
                        lbs_12w=('Pounds_CY','sum'),
                        avg_margin_per_lb=('margin_per_lb','mean'))
                    .reset_index())
        by_cust['repeat_flag'] = (by_cust['weeks_active'] >= 2).astype(int)

        # Recency: weeks since last purchase in window
        last_wk = (cw.groupby(cust_keys, dropna=False)['Fiscal Week Number'].max()
                    .rename('last_wk').reset_index())
        by_cust = by_cust.merge(last_wk, on=cust_keys, how='left')
        if by_cust['last_wk'].notna().any():
            max_wk = pd.to_numeric(cw['Fiscal Week Number'], errors='coerce').max()
            by_cust['recency_weeks'] = (max_wk - by_cust['last_wk']).clip(lower=0)
        else:
            by_cust['recency_weeks'] = np.nan

        # Retention over last 4 weeks: active both in last 4w and earlier in window
        recent_cut = max_wk - 3 if pd.notna(max_wk) else None
        if recent_cut is not None:
            recent = cw[cw['Fiscal Week Number'] >= recent_cut]
            early  = cw[cw['Fiscal Week Number'] <  recent_cut]
            rec_a = recent.groupby(cust_keys, dropna=False)['Fiscal Week Number'].nunique().rename('recent_weeks')
            rec_b = early.groupby(cust_keys, dropna=False)['Fiscal Week Number'].nunique().rename('early_weeks')
            tmp = rec_a.to_frame().merge(rec_b, left_index=True, right_index=True, how='left').fillna(0)
            tmp['retained_4w_flag'] = ((tmp['recent_weeks'] > 0) & (tmp['early_weeks'] > 0)).astype(int)
            by_cust = by_cust.merge(tmp['retained_4w_flag'].reset_index(), on=cust_keys, how='left').fillna({'retained_4w_flag':0})
        else:
            by_cust['retained_4w_flag'] = 0

        # Aggregate to combo+zone level
        zkeys = ['Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type','Zone_Suffix_Numeric']
        Z = (by_cust.groupby(zkeys, dropna=False)
                    .agg(active_customers=('Company Customer Number','nunique'),
                        repeat_buyers=('repeat_flag','sum'),
                        avg_freq=('purchases_12w','mean'),
                        avg_recency=('recency_weeks','mean'),
                        retention_4w=('retained_4w_flag','mean'),
                        avg_margin_per_lb=('avg_margin_per_lb','mean'),
                        lbs=('lbs_12w','sum'))
                    .reset_index())
        Z['repeat_buyer_rate'] = np.where(Z['active_customers']>0, Z['repeat_buyers']/Z['active_customers'], 0.0)
        Z['active_buyer_share'] = np.where(Z['active_customers']>0, 1.0, 0.0)  # presence indicator in window

        # Winsorize margin for stability
        if 'avg_margin_per_lb' in Z.columns:
            Z['avg_margin_per_lb'] = self._winsorize(Z['avg_margin_per_lb'])

        # Normalize inputs to 0–1 for scoring
        # Note: for recency, lower is better → we invert after clamping
        Z['nx_repeat']  = Z['repeat_buyer_rate'].clip(0,1)
        Z['nx_active']  = Z['active_buyer_share'].clip(0,1)
        Z['nx_ret4w']   = Z['retention_4w'].clip(0,1)
        Z['nx_freq']    = (Z['avg_freq'] / max(self.behavior_target_freq, 1)).clip(0,1)
        Z['nx_recency'] = (1 - (Z['avg_recency'] / max(self.behavior_recency_cap,1))).clip(0,1)
        Z['nx_mplb']    = self._normalize_01(Z['avg_margin_per_lb'].fillna(Z['avg_margin_per_lb'].median()))

        # Behavior score (margin stays in as a small, normalized contributor)
        w = self.behavior_weights
        Z['BehaviorScore'] = (
            w['repeat_buyer_rate'] * Z['nx_repeat'] +
            w['active_buyer_share'] * Z['nx_active'] +
            w['retention_4w']      * Z['nx_ret4w'] +
            w['avg_freq']          * Z['nx_freq'] +
            w['recency']           * Z['nx_recency'] +
            w['margin_per_lb']     * Z['nx_mplb']
        ).clip(0, 1)

        return Z

    def __init__(self, learning_window_weeks=26, min_sample_size=3, historical_analysis_mode=True):
        

        self.learning_window_weeks = learning_window_weeks
        self.min_sample_size = min_sample_size
        self.historical_analysis_mode = historical_analysis_mode  # NEW: Enable historical analysis
        
                    # === Behavior weighting knobs (tune if needed) ===
        self.behavior_weights = {
            "repeat_buyer_rate": 0.40,
            "active_buyer_share": 0.15,
            "retention_4w": 0.15,
            "avg_freq": 0.10,
            "recency": 0.10,         # lower recency (fresher) is better
            "margin_per_lb": 0.10    # keep margin in the mix, lightly weighted
        }
        self.behavior_window_weeks = 12
        self.behavior_recency_cap = 12  # clamp recency contribution
        self.behavior_target_freq = 4   # 4 active weeks in 12w window ~ monthly buyer
        self.allow_upzone_epsilon = 0.01  # tolerance for "non-inferior"
        
        # Core learning structures for zone performance
        self.zone_performance = defaultdict(lambda: {
            'volume_elasticity': 0.0,
            'customer_elasticity': 0.0,
            'optimal_zone_estimate': 5.0,
            'zone_effectiveness_curve': {},
            'sample_count': 0,
            'confidence': 0.0,
            'last_updated': datetime.now(),
            'historical_performance': []
        })
        
        # NEW: Historical pattern storage
        self.historical_patterns = defaultdict(list)
        self.proven_zone_changes = {}
        self.failed_zone_changes = {}
        self.high_confidence_recommendations = []
        
        # Track zone change outcomes
        self.zone_change_outcomes = deque(maxlen=500)
        
        # Granularity expansion tracking
        self.granularity_needs = defaultdict(lambda: {
            'needs_expansion': False,
            'recommended_zones_between_0_1': 0,
            'zero_recommendations_count': 0,
            'evidence_score': 0.0
        })
        
        # Price sensitivity learning
        self.price_sensitivity_patterns = defaultdict(list)
    

    def analyze_historical_zone_changes(self, full_historical_df: pd.DataFrame) -> Dict:
        """
        Vectorized: analyze 2+ years of historical data to identify proven zone change patterns
        MUCH faster than looping over each Company_Combo_Key.
        """
        print("Analyzing historical zone change patterns...")
        print(f"rows={len(full_historical_df):,}, combos={full_historical_df['Company Name'].nunique()} companies")


        df = self.extract_zone_features(full_historical_df).copy()

        # Required columns sanity
        for col in ['Pounds_CY', 'Combined_Performance_Score', 'Volume_Growth_Rate', 'Zone_Suffix_Numeric']:
            if col not in df.columns:
                df[col] = 0

        # Ensure numerics (avoid '>' between str and int issues later)
        num_cols = ['Pounds_CY', 'Combined_Performance_Score', 'Volume_Growth_Rate', 'Zone_Suffix_Numeric']
        for c in num_cols:
            df[c] = pd.to_numeric(df[c], errors='coerce')
        df['Pounds_CY'] = df['Pounds_CY'].fillna(0)
        df['Combined_Performance_Score'] = df['Combined_Performance_Score'].fillna(0)
        df['Volume_Growth_Rate'] = df['Volume_Growth_Rate'].fillna(0)
        # Remove unknown zone rows
        df = df[df['Zone_Suffix_Numeric'].between(0, 20, inclusive='both')]  # or <=5 if you’re hard-capped


        # We’ll treat each row as a "week". If you have true week IDs, we’ll use them to count weeks.
        if 'Fiscal Week Number' in df.columns:
            df['Fiscal Week Number'] = pd.to_numeric(df['Fiscal Week Number'], errors='coerce')

        # 1) Aggregate per (Company_Combo_Key, Zone)
        grp = df.groupby(['Company_Combo_Key', 'Zone_Suffix_Numeric'], dropna=False).agg(
            avg_performance=('Combined_Performance_Score', 'mean'),
            total_volume=('Pounds_CY', 'sum'),
            weeks_tested=('Fiscal Week Number', 'nunique' if 'Fiscal Week Number' in df.columns else 'size'),
            volume_growth_rate=('Volume_Growth_Rate', 'mean'),
            company_name=('Company Name', 'first'),
            combo_key=('Combo_Key', 'first')
        ).reset_index()

        # filter out zones with too little data (optional but helps stability)
        grp = grp[grp['weeks_tested'] >= 3]

        # 2) Compute per-company totals
        company_totals = grp.groupby('Company_Combo_Key', dropna=False).agg(
            total_historical_volume=('total_volume', 'sum'),
            zones_tested_count=('Zone_Suffix_Numeric', 'nunique'),
            total_weeks=('weeks_tested', 'sum')
        ).reset_index()

        # 3) Identify best zone per company (by avg_performance)
        idx_best = grp.groupby('Company_Combo_Key')['avg_performance'].idxmax()
        best = grp.loc[idx_best].rename(columns={
            'Zone_Suffix_Numeric': 'historical_best_zone',
            'avg_performance': 'historical_best_performance'
        })

        # 4) For confidence & pattern type, we need the "second best" and separation
        #    Compute average of the non-best zones per company
        merged = grp.merge(best[['Company_Combo_Key', 'historical_best_zone']], on='Company_Combo_Key', how='left')
        merged['is_best'] = merged['Zone_Suffix_Numeric'] == merged['historical_best_zone']

        others_avg = (merged[~merged['is_best']]
                    .groupby('Company_Combo_Key', dropna=False)['avg_performance']
                    .mean()
                    .rename('others_avg_performance')
                    .reset_index())

        out = (best.merge(company_totals, on='Company_Combo_Key', how='left')
                    .merge(others_avg, on='Company_Combo_Key', how='left'))

        # fill NaN others_avg_performance (cases with only one zone tested)
        out['others_avg_performance'] = out['others_avg_performance'].fillna(out['historical_best_performance'])

        # Confidence calculation (match your earlier logic)
        def _conf(row):
            zone_conf = min(row['zones_tested_count'] / 5.0, 0.4)
            time_conf = min(row['total_weeks'] / 50.0, 0.4)
            separation = abs(row['historical_best_performance'] - row['others_avg_performance'])
            sep_conf = min(separation / 0.3, 0.2)
            return min(zone_conf + time_conf + sep_conf, 0.95)

        out['confidence'] = out.apply(_conf, axis=1)

        # Proven pattern classification
        def _pattern(row):
            if row['historical_best_zone'] <= 1:
                return "GRANULARITY_EXPANSION_VALIDATED"
            separation = row['historical_best_performance'] - row['others_avg_performance']
            if row['zones_tested_count'] >= 3:
                return "CLEAR_OPTIMAL_ZONE" if separation > 0.2 else "MARGINAL_DIFFERENCES"
            return "LIMITED_COMPARISON"

        out['proven_pattern'] = out.apply(_pattern, axis=1)

        # 5) Rebuild the dict structure you expect
        hist_patterns = {}
        # Also collect the list of zones tested per company (vectorized)
        zones_list = (grp.groupby('Company_Combo_Key')['Zone_Suffix_Numeric']
                        .apply(lambda s: sorted(s.unique().tolist()))
                        .rename('zones_tested').reset_index())

        # merge zones list and a convenient dict-ish snapshot of zone history (optional)
        out = out.merge(zones_list, on='Company_Combo_Key', how='left')

        # (Optional) Keep the full per-zone records (history) per company for reference
        zone_perf_records = (grp.groupby('Company_Combo_Key')
                            .apply(lambda g: g.sort_values('avg_performance', ascending=False)
                                    .to_dict('records'))
                            .rename('zone_performance_history')
                            .reset_index())
        out = out.merge(zone_perf_records, on='Company_Combo_Key', how='left')

        for _, r in out.iterrows():
            hist_patterns[r['Company_Combo_Key']] = {
                'company_name': r['company_name'],
                'combo_key': r['combo_key'],
                'total_historical_volume': float(r['total_historical_volume']),
                'zones_tested': r['zones_tested'],
                'historical_best_zone': float(r['historical_best_zone']),
                'historical_best_performance': float(r['historical_best_performance']),
                'zone_performance_history': r['zone_performance_history'],
                'confidence': float(r['confidence']),
                'proven_pattern': r['proven_pattern'],
            }

        self.historical_patterns = hist_patterns
        print(f"Historical patterns found for {len(hist_patterns):,} combos")
        return hist_patterns

    
    def _calculate_historical_confidence(self, zone_performance_history: List[Dict]) -> float:
        """Calculate confidence based on historical data depth and consistency"""
        
        if not zone_performance_history:
            return 0.0
        
        # More zones tested = higher confidence
        zones_tested = len(zone_performance_history)
        zone_confidence = min(zones_tested / 5.0, 0.4)  # Max 40% from variety
        
        # More data points = higher confidence  
        total_weeks = sum(zp['weeks_tested'] for zp in zone_performance_history)
        time_confidence = min(total_weeks / 50.0, 0.4)  # Max 40% from sample size
        
        # Performance separation = higher confidence
        if len(zone_performance_history) >= 2:
            best_performance = zone_performance_history[0]['avg_performance']
            second_best = zone_performance_history[1]['avg_performance']
            separation = abs(best_performance - second_best)
            separation_confidence = min(separation / 0.3, 0.2)  # Max 20% from clear winner
        else:
            separation_confidence = 0.0
        
        return min(zone_confidence + time_confidence + separation_confidence, 0.95)
    
    def _identify_proven_pattern(self, zone_performance_history: List[Dict]) -> str:
        """Identify the type of proven pattern from historical data"""
        
        if not zone_performance_history:
            return "INSUFFICIENT_DATA"
        
        zones_by_performance = [zp['zone'] for zp in zone_performance_history]
        best_zone = zones_by_performance[0]
        
        # Check for Zone 0 tendency
        if best_zone <= 1:
            return "GRANULARITY_EXPANSION_VALIDATED"
        
        # Check for clear zone preference
        if len(zone_performance_history) >= 3:
            best_performance = zone_performance_history[0]['avg_performance']
            avg_others = np.mean([zp['avg_performance'] for zp in zone_performance_history[1:]])
            
            if best_performance > avg_others + 0.2:
                return "CLEAR_OPTIMAL_ZONE"
            else:
                return "MARGINAL_DIFFERENCES"
        
        return "LIMITED_COMPARISON"
    
    def generate_immediate_high_confidence_recommendations(self, current_data_df: pd.DataFrame) -> List[Dict]:
        """
        Generate immediate recommendations from historical patterns,
        anchored on BehaviorScore and guarded against harmful up-zoning.
        """
        # If we don't have learned historical patterns, fall back to standard analysis
        if not getattr(self, "historical_patterns", None):
            zone_df = self.extract_zone_features(current_data_df)
            zone_analysis = self.analyze_zone_effectiveness(zone_df)
            return self.generate_zone_recommendations(zone_analysis)

        print("Generating high-confidence historical recommendations...")

        # Extract current features (includes BehaviorScore and behavior metrics if you added the patch)
        current_df = self.extract_zone_features(current_data_df).copy()

        # Helper to safely fetch a metric for a given combo+zone
        def _m(combo_key, zone_num, col, default=np.nan):
            rows = current_df[(current_df['Company_Combo_Key'] == combo_key) &
                            (current_df['Zone_Suffix_Numeric'] == zone_num)]
            if rows.empty or col not in rows.columns or pd.isna(rows.iloc[0][col]):
                return default
            return float(rows.iloc[0][col])

        recs: List[Dict] = []

        for company_combo, hist in self.historical_patterns.items():
            # Skip combos not present in current snapshot
            cur_rows = current_df[current_df['Company_Combo_Key'] == company_combo]
            if cur_rows.empty:
                continue

            # Current & recommended zones
            current_zone = float(cur_rows['Zone_Suffix_Numeric'].iloc[0])
            recommended_zone = float(hist.get('historical_best_zone', current_zone))
            confidence = float(hist.get('confidence', 0.0))

            # Only consider clear, meaningful changes
            if confidence < 0.70 or abs(current_zone - recommended_zone) < 1:
                continue

            # Behavior-first performance deltas (fallback to Combined if BehaviorScore missing)
            cur_beh = _m(company_combo, current_zone, 'BehaviorScore', np.nan)
            rec_beh = _m(company_combo, recommended_zone, 'BehaviorScore', np.nan)
            if np.isnan(cur_beh) or np.isnan(rec_beh):
                cur_beh = _m(company_combo, current_zone, 'Combined_Performance_Score', 0.0)
                rec_beh = _m(company_combo, recommended_zone, 'Combined_Performance_Score', 0.0)

            beh_improvement = rec_beh - cur_beh

            # Volume & margin context (optional for messaging/priority)
            current_volume = float(cur_rows.get('Pounds_CY', pd.Series([0])).sum())
            hist_best_perf = float(hist.get('historical_best_performance', rec_beh))
            expected_improvement = hist_best_perf - cur_beh  # used by existing priority fn

            # --- BEHAVIOR GUARDRAIL FOR UP-ZONE ---
            allow = True
            if recommended_zone > current_zone:
                eps = ALLOW_UPZONE_EPSILON
                rb_curr = _m(company_combo, current_zone, 'repeat_buyer_rate', 0.0)
                rb_rec  = _m(company_combo, recommended_zone, 'repeat_buyer_rate', 0.0)
                rt_curr = _m(company_combo, current_zone, 'retention_4w', 0.0)
                rt_rec  = _m(company_combo, recommended_zone, 'retention_4w', 0.0)
                ab_curr = _m(company_combo, current_zone, 'active_buyer_share', 0.0)
                ab_rec  = _m(company_combo, recommended_zone, 'active_buyer_share', 0.0)

                non_inferior = (
                    (rb_rec + eps >= rb_curr) and
                    (rt_rec + eps >= rt_curr) and
                    (ab_rec + eps >= ab_curr)
                )
                allow = bool(non_inferior)

            # Build recommendation object(s)
            if allow and beh_improvement > 0:
                rec = {
                    'type': 'HISTORICAL_PROVEN',
                    'recommendation_type': 'HISTORICAL_PROVEN',
                    'company_combo': company_combo,
                    'company_name': hist.get('company_name'),
                    'combo_description': hist.get('combo_key'),
                    'current_zone': current_zone,
                    'recommended_zone': recommended_zone,
                    'confidence': confidence,
                    'historical_evidence': f"Historically optimal zone with {confidence:.0%} confidence (behavior-first).",
                    # keep your existing estimator (maps perf delta to % lift); behavior drives the delta
                    'expected_volume_impact': self._estimate_historical_volume_impact(expected_improvement, current_volume),
                    'total_volume_at_stake': current_volume,
                    'implementation_priority': self._calculate_historical_priority(confidence, current_volume, expected_improvement),
                    'expected_timeline': "2-3 weeks (historically proven pattern)",
                    'risk_assessment': self._assess_historical_risk(hist, current_zone, recommended_zone),
                    'proven_pattern': hist.get('proven_pattern'),
                    'historical_zones_tested': hist.get('zones_tested'),
                    'specific_actions': [f"Change from Zone {current_zone} to Zone {recommended_zone}"],
                    # expose behavior metrics for transparency
                    'behavior_current': {
                        'BehaviorScore': cur_beh,
                        'repeat_buyer_rate': _m(company_combo, current_zone, 'repeat_buyer_rate', np.nan),
                        'retention_4w': _m(company_combo, current_zone, 'retention_4w', np.nan),
                        'active_buyer_share': _m(company_combo, current_zone, 'active_buyer_share', np.nan),
                    },
                    'behavior_recommended': {
                        'BehaviorScore': rec_beh,
                        'repeat_buyer_rate': _m(company_combo, recommended_zone, 'repeat_buyer_rate', np.nan),
                        'retention_4w': _m(company_combo, recommended_zone, 'retention_4w', np.nan),
                        'active_buyer_share': _m(company_combo, recommended_zone, 'active_buyer_share', np.nan),
                    },
                }
                recs.append(rec)
            else:
                # Do not auto up-zone; downgrade to investigate (or consider down-zone A/B)
                recs.append({
                    'type': 'INVESTIGATE_OVERPRICING_RISK',
                    'recommendation_type': 'INVESTIGATE',
                    'company_combo': company_combo,
                    'company_name': hist.get('company_name'),
                    'combo_description': hist.get('combo_key'),
                    'current_zone': current_zone,
                    'recommended_zone': recommended_zone,
                    'confidence': confidence,
                    'historical_evidence': "Historical best conflicts with behavior (repeat/retention/active).",
                    'expected_volume_impact': "Behavior not supportive; test lower/equal zone first.",
                    'total_volume_at_stake': current_volume,
                    'implementation_priority': max(
                        0.0,
                        self._calculate_historical_priority(confidence, current_volume, expected_improvement) - 25.0
                    ),
                    'expected_timeline': "Investigate",
                    'risk_assessment': {
                        'risk_level': 'MEDIUM',
                        'risk_factors': ['Behavior metrics worse or non-inferior on higher suffix'],
                        'mitigation_strategies': ['Run A/B with equal/lower zone for 2–4 weeks; monitor repeat & retention.']
                    },
                    'proven_pattern': hist.get('proven_pattern'),
                    'historical_zones_tested': hist.get('zones_tested'),
                    'specific_actions': ['Investigate overpricing; consider down-zone pilot/A-B.'],
                    'behavior_current': {
                        'BehaviorScore': cur_beh,
                        'repeat_buyer_rate': _m(company_combo, current_zone, 'repeat_buyer_rate', np.nan),
                        'retention_4w': _m(company_combo, current_zone, 'retention_4w', np.nan),
                        'active_buyer_share': _m(company_combo, current_zone, 'active_buyer_share', np.nan),
                    },
                    'behavior_recommended': {
                        'BehaviorScore': rec_beh,
                        'repeat_buyer_rate': _m(company_combo, recommended_zone, 'repeat_buyer_rate', np.nan),
                        'retention_4w': _m(company_combo, recommended_zone, 'retention_4w', np.nan),
                        'active_buyer_share': _m(company_combo, recommended_zone, 'active_buyer_share', np.nan),
                    },
                })

        # Order by priority (desc)
        recs.sort(key=lambda x: x.get('implementation_priority', 0), reverse=True)
        print(f"Generated {len(recs)} high-confidence historical recommendations")
        return recs

    
    def _estimate_historical_volume_impact(self, performance_improvement: float, current_volume: float) -> str:
        def fmt(lbs): return f"{lbs:,.0f} lbs"
        if performance_improvement > 0.3: return f"20–30% volume increase (~{fmt(current_volume * 0.25)})"
        if performance_improvement > 0.2: return f"15–20% volume increase (~{fmt(current_volume * 0.175)})"
        if performance_improvement > 0.1: return f"10–15% volume increase (~{fmt(current_volume * 0.125)})"
        if performance_improvement > 0:   return f"5–10% volume increase (~{fmt(current_volume * 0.075)})"
        return "Minimal impact expected"

    
    def _calculate_historical_priority(self, confidence: float, volume: float, improvement: float) -> float:
        """Calculate priority for historical recommendations"""
        
        priority = 0.0
        
        # Confidence weight (0-40 points)
        priority += confidence * 40
        
        # Volume weight (0-30 points)  
        priority += min(volume / 10000, 30)
        
        # Improvement potential (0-30 points)
        priority += min(improvement * 100, 30)
        
        return min(priority, 100)
    
    def _assess_historical_risk(self, historical_analysis: Dict, current_zone: float, recommended_zone: float) -> Dict:
        """Assess risk based on historical patterns"""
        
        risk_factors = []
        risk_level = "LOW"
        
        # Large zone change risk
        zone_change = abs(current_zone - recommended_zone)
        if zone_change > 2:
            risk_factors.append(f"Large zone change: {current_zone} to {recommended_zone}")
            risk_level = "MEDIUM"
        
        # High volume risk
        if historical_analysis['total_historical_volume'] > 50000:
            risk_factors.append("High historical volume - significant business impact")
            risk_level = "MEDIUM"
        
        # Pattern reliability
        if historical_analysis['proven_pattern'] == "MARGINAL_DIFFERENCES":
            risk_factors.append("Historically marginal differences between zones")
            if risk_level == "LOW":
                risk_level = "MEDIUM"
        
        # Confidence-based risk adjustment
        if historical_analysis['confidence'] > 0.9:
            risk_level = "LOW"  # Override to low risk for very high confidence
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'mitigation_strategies': self._historical_risk_mitigation(risk_factors, historical_analysis)
        }
    
    def _historical_risk_mitigation(self, risk_factors: List[str], historical_analysis: Dict) -> List[str]:
        """Suggest risk mitigation for historical recommendations"""
        
        mitigations = []
        
        if any("Large zone change" in factor for factor in risk_factors):
            mitigations.append("Phase implementation: test intermediate zone first for 1 week")
        
        if any("High historical volume" in factor for factor in risk_factors):
            mitigations.append("Monitor daily for first 5 days, weekly thereafter")
        
        if any("marginal differences" in factor for factor in risk_factors):
            mitigations.append("Set clear success metrics and 2-week evaluation checkpoint")
        
        # Always include historical context
        zones_tested = ", ".join(map(str, historical_analysis['zones_tested']))
        mitigations.append(f"Historical context: Previously tested zones {zones_tested}")
        
        return mitigations
    
    def extract_zone_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract zone optimization features from data"""
        
        zone_df = df.copy()
        
        # Ensure numeric dtypes (CSV loads often give strings)
        for c in ['Pounds_CY', 'Pounds_PY', 'Delta_YoY_Lbs', 'Distinct_Customers_CY', 'Distinct_Customers_PY']:
            if c in zone_df.columns:
                zone_df[c] = pd.to_numeric(zone_df[c], errors='coerce').fillna(0)

        
        # Parse Price Zone ID to get Zone Suffix
        if 'Price Zone ID' in zone_df.columns:
            zone_df['Zone_Suffix'] = zone_df['Price Zone ID'].astype(str).str.split('-').str[-1]
            zone_df['Zone_Suffix_Numeric'] = pd.to_numeric(zone_df['Zone_Suffix'], errors='coerce').fillna(999)
        else:
            # If no Price Zone ID, try to infer from other columns or create dummy
            zone_df['Zone_Suffix'] = '5'  # Default middle zone
            zone_df['Zone_Suffix_Numeric'] = 5
        
        # Create the significant combo key
        zone_df['Combo_Key'] = (
            zone_df.get('NPD Cuisine Type', 'Unknown').astype(str) + '_' +
            zone_df.get('Attribute Group ID', 'Unknown').astype(str) + '_' +
            zone_df.get('Price Source Type', 'Unknown').astype(str)
        )
        
        # Company-specific combo key
        zone_df['Company_Combo_Key'] = (
            zone_df.get('Company Name', 'Unknown').astype(str) + '_' + zone_df['Combo_Key']
        )
        
        # Performance metrics
        zone_df['Volume_Performance'] = zone_df.get('Delta_YoY_Lbs', 0)
        zone_df['Volume_Growth_Rate'] = np.where(
            zone_df.get('Pounds_PY', 0) > 0,
            zone_df.get('Delta_YoY_Lbs', 0) / zone_df.get('Pounds_PY', 1),
            0
        )
        
        # Customer metrics (if available)
        if 'Distinct_Customers_CY' in zone_df.columns and 'Distinct_Customers_PY' in zone_df.columns:
            zone_df['Customer_Growth'] = zone_df['Distinct_Customers_CY'] - zone_df['Distinct_Customers_PY']
            zone_df['Customer_Growth_Rate'] = np.where(
                zone_df['Distinct_Customers_PY'] > 0,
                zone_df['Customer_Growth'] / zone_df['Distinct_Customers_PY'],
                0
            )
        else:
            # Estimate distinct customers from volume patterns
            zone_df['Customer_Growth'] = zone_df['Volume_Performance'] * 0.1  # Rough proxy
            zone_df['Customer_Growth_Rate'] = zone_df['Volume_Growth_Rate'] * 0.8  # Customers less elastic
        
        # Combined performance score (weighted toward volume and customers)
        zone_df['Combined_Performance_Score'] = (
            zone_df['Volume_Growth_Rate'] * 0.6 +  # Volume gets 60% weight
            zone_df['Customer_Growth_Rate'] * 0.4   # Customers get 40% weight
        )
        
        # === BEHAVIOR SIGNALS MERGE ===
        behavior = self._compute_behavior_signals(zone_df, window_weeks=self.behavior_window_weeks)

        if not behavior.empty:
            merge_keys = ['Company Name','NPD Cuisine Type','Attribute Group ID','Price Source Type','Zone_Suffix_Numeric']
            # Ensure keys exist
            for k in merge_keys:
                if k not in zone_df.columns:
                    zone_df[k] = 'UNKNOWN'
            zone_df = zone_df.merge(behavior, on=merge_keys, how='left', suffixes=('',''))

            # If BehaviorScore is missing (e.g., thin data), backfill with Combined_Performance_Score scaled to 0–1
            if 'BehaviorScore' not in zone_df.columns or zone_df['BehaviorScore'].isna().all():
                # normalize combined to 0-1 as fallback, keep your original meaning
                tmp = self._normalize_01(zone_df['Combined_Performance_Score'].fillna(0))
                zone_df['BehaviorScore'] = tmp.fillna(0)

        # Keep a clean float
        zone_df['BehaviorScore'] = pd.to_numeric(zone_df['BehaviorScore'], errors='coerce').fillna(0.0)

        return zone_df
    
    def analyze_zone_effectiveness(self, zone_df: pd.DataFrame) -> Dict:
        """Analyze how different zones perform for each company-combo"""
        
        zone_analysis = {}
        
        for company_combo in zone_df['Company_Combo_Key'].unique():
            
            combo_data = zone_df[zone_df['Company_Combo_Key'] == company_combo].copy()
            
            if len(combo_data) < 2:  # Need multiple data points
                continue
            
            # Group by zone suffix to see performance patterns
            zone_performance = combo_data.groupby('Zone_Suffix_Numeric').agg({
                'Volume_Performance': 'sum',
                'Volume_Growth_Rate': 'mean',
                'Customer_Growth': 'sum',
                'Customer_Growth_Rate': 'mean',
                'Combined_Performance_Score': 'mean',
                'BehaviorScore': 'mean',                         # NEW
                'repeat_buyer_rate': 'mean',                     # NEW
                'active_buyer_share': 'mean',                    # NEW
                'avg_freq': 'mean', 'avg_recency': 'mean',       # NEW
                'retention_4w': 'mean', 'avg_margin_per_lb': 'mean',  # NEW
                'Pounds_CY': 'sum',
                'Company Name': 'first',
                'Combo_Key': 'first'
            }).reset_index()

            zone_performance = zone_performance.sort_values('Zone_Suffix_Numeric')

            # Choose optimal by BEHAVIOR, not margin/combined
            if not zone_performance.empty:
                best_zone_idx = zone_performance['BehaviorScore'].idxmax()
                optimal_zone = zone_performance.loc[best_zone_idx, 'Zone_Suffix_Numeric']
                best_performance = zone_performance.loc[best_zone_idx, 'BehaviorScore']  # store behavior score as performance

                
                # Calculate price elasticity (how performance changes with zone)
                if len(zone_performance) >= 3:
                    zones = zone_performance['Zone_Suffix_Numeric'].values
                    scores = zone_performance['Combined_Performance_Score'].values
                    
                    # Simple elasticity: change in performance per zone change
                    zone_changes = np.diff(zones)
                    score_changes = np.diff(scores)
                    elasticities = np.where(zone_changes != 0, score_changes / zone_changes, 0)
                    avg_elasticity = np.mean(elasticities)
                else:
                    avg_elasticity = 0
                
                zone_analysis[company_combo] = {
                    'company_name': combo_data['Company Name'].iloc[0],
                    'combo_key': combo_data['Combo_Key'].iloc[0],
                    'current_zones_tested': zone_performance['Zone_Suffix_Numeric'].tolist(),
                    'zone_performance_curve': zone_performance.to_dict('records'),
                    'optimal_zone': optimal_zone,
                    'optimal_performance': best_performance,
                    'price_elasticity': avg_elasticity,
                    'total_volume': combo_data['Pounds_CY'].sum(),
                    'needs_analysis': self._assess_analysis_needs(zone_performance, optimal_zone),
                    'zone_performance_curve': zone_performance.to_dict('records'),
                    'optimal_zone': optimal_zone,
                    'optimal_performance': float(best_performance),
                    'behavior_best': {
                        'repeat_buyer_rate': float(zone_performance.loc[best_zone_idx, 'repeat_buyer_rate']),
                        'retention_4w': float(zone_performance.loc[best_zone_idx, 'retention_4w']),
                        'active_buyer_share': float(zone_performance.loc[best_zone_idx, 'active_buyer_share']),
                        'avg_freq': float(zone_performance.loc[best_zone_idx, 'avg_freq']),
                        'avg_recency': float(zone_performance.loc[best_zone_idx, 'avg_recency']),
                        'avg_margin_per_lb': float(zone_performance.loc[best_zone_idx, 'avg_margin_per_lb'])
                    },
                }
        
        return zone_analysis
    
    def _assess_analysis_needs(self, zone_performance: pd.DataFrame, optimal_zone: float) -> Dict:
        """Assess what kind of analysis or action is needed for this combo"""
        
        needs = {
            'type': 'STABLE',
            'recommendations': [],
            'confidence': 'HIGH',
            'priority': 'LOW'
        }
        
        zones_tested = set(zone_performance['Zone_Suffix_Numeric'].tolist())
        performance_scores = zone_performance['Combined_Performance_Score'].tolist()
        
        # Check if optimal zone is at boundary (need to test more zones)
        min_zone_tested = min(zones_tested)
        max_zone_tested = max(zones_tested)
        
        if optimal_zone == min_zone_tested and min_zone_tested > 0:
            needs['type'] = 'TEST_LOWER_ZONES'
            needs['recommendations'].append(f"Test zones {max(0, min_zone_tested-2)}-{min_zone_tested-1}")
            needs['confidence'] = 'MEDIUM'
            needs['priority'] = 'MEDIUM'
            
            # Special case: if optimal is zone 1 or 2, flag for granularity expansion
            if optimal_zone <= 2:
                needs['granularity_expansion_candidate'] = True
                needs['recommendations'].append("Consider adding zones 0.1, 0.2, 0.5 for finer pricing")
        
        elif optimal_zone == max_zone_tested:
            needs['type'] = 'TEST_HIGHER_ZONES' 
            needs['recommendations'].append(f"Test zones {max_zone_tested+1}-{max_zone_tested+3}")
            needs['confidence'] = 'MEDIUM'
            needs['priority'] = 'LOW'  # Less urgent since we're not at the cheap end
        
        # Check if we're recommending zone 0 (which we won't implement)
        if optimal_zone == 0:
            needs['type'] = 'GRANULARITY_EXPANSION_NEEDED'
            needs['recommendations'] = [
                "ZONE 0 RECOMMENDED - DO NOT IMPLEMENT",
                "Create zones 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 between current 0 and 1",
                "This combo shows high price sensitivity - needs finer pricing granularity"
            ]
            needs['confidence'] = 'HIGH'
            needs['priority'] = 'HIGH'
        
        # Check for strong price sensitivity (big performance differences between zones)
        if len(performance_scores) >= 3:
            performance_range = max(performance_scores) - min(performance_scores)
            if performance_range > 0.3:  # Large performance swing across zones
                needs['high_price_sensitivity'] = True
                if needs['type'] == 'STABLE':
                    needs['type'] = 'HIGH_SENSITIVITY'
                    needs['recommendations'].append("Monitor closely - high price sensitivity detected")
                    needs['priority'] = 'MEDIUM'
        
        return needs
    
    def generate_zone_recommendations(self, zone_analysis: Dict) -> List[Dict]:
        """Generate specific zone change recommendations"""
        
        recommendations = []
        
        for company_combo, analysis in zone_analysis.items():
            
            needs = analysis['needs_analysis']
            current_optimal = analysis['optimal_zone']
            total_volume = analysis['total_volume']
            
            # Skip low-volume combos unless they show exceptional promise
            if total_volume < 1000 and analysis['optimal_performance'] < 0.2:
                continue
            
            rec = {
                'company_combo': company_combo,
                'company_name': analysis['company_name'],
                'combo_description': analysis['combo_key'],
                'current_optimal_zone': current_optimal,
                'current_performance': analysis['optimal_performance'],
                'total_volume_at_stake': total_volume,
                'recommendation_type': needs['type'],
                'specific_actions': needs['recommendations'],
                'implementation_priority': self._calculate_zone_priority(analysis, needs),
                'expected_timeline': self._estimate_implementation_timeline(needs),
                'risk_assessment': self._assess_zone_change_risk(analysis, needs)
            }
            
            # Add specific zone targets for actionable recommendations
            if needs['type'] == 'TEST_LOWER_ZONES':
                suggested_zones = self._suggest_specific_lower_zones(analysis)
                rec['suggested_test_zones'] = suggested_zones
                rec['expected_volume_impact'] = self._estimate_volume_impact(analysis, suggested_zones)
            
            elif needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
                rec['granularity_expansion_plan'] = self._create_granularity_plan(analysis)
                rec['expected_volume_impact'] = "15-30% increase with proper granularity"
            
            recommendations.append(rec)
        
        return sorted(recommendations, key=lambda x: x['implementation_priority'], reverse=True)
    
    def _calculate_zone_priority(self, analysis: Dict, needs: Dict) -> float:
        """Calculate implementation priority (0-100)"""
        
        priority = 0.0
        
        # Volume impact potential
        volume_factor = min(analysis['total_volume'] / 10000, 30)  # Max 30 points
        priority += volume_factor
        
        # Performance opportunity
        if analysis['optimal_performance'] > 0.1:
            priority += 25  # Good performance opportunity
        elif analysis['optimal_performance'] > 0.05:
            priority += 15
        
        # Urgency based on need type
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            priority += 35  # High urgency - zone 0 recommendations
        elif needs['type'] == 'TEST_LOWER_ZONES':
            priority += 20  # Medium urgency - potential for lower pricing
        elif needs['type'] == 'HIGH_SENSITIVITY':
            priority += 10  # Monitor closely
        
        # Price elasticity bonus (more responsive = higher priority)
        if abs(analysis['price_elasticity']) > 0.1:
            priority += 10
        
        return min(priority, 100)
    
    def _suggest_specific_lower_zones(self, analysis: Dict) -> List[int]:
        """Suggest specific lower zones to test"""
        
        current_optimal = analysis['optimal_zone']
        zones_tested = set(analysis['current_zones_tested'])
        
        suggestions = []
        
        # Test 2-3 zones below current optimal
        for test_zone in range(max(0, int(current_optimal) - 3), int(current_optimal)):
            if test_zone not in zones_tested and test_zone >= 0:
                suggestions.append(test_zone)
        
        return suggestions[:3]  # Limit to 3 suggestions
    
    def _create_granularity_plan(self, analysis: Dict) -> Dict:
        """Create a plan for adding granular zones between 0 and 1"""
        
        return {
            'problem': 'Optimal zone recommendation is 0, but we will not implement zone 0',
            'solution': 'Add granular zones between 0 and 1',
            'suggested_new_zones': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
            'implementation_approach': 'Start with 0.5 and 0.8, then add others based on results',
            'expected_outcome': 'Find sweet spot that maximizes volume without going to zone 0',
            'monitoring_plan': 'Test 0.5 for 4 weeks, then 0.8 for 4 weeks, compare to current zone 1'
        }
    
    def _estimate_volume_impact(self, analysis: Dict, suggested_zones: List[int]) -> str:
        """Estimate volume impact from testing suggested zones"""
        
        elasticity = analysis['price_elasticity']
        current_optimal = analysis['optimal_zone']
        
        if not suggested_zones:
            return "Unable to estimate - no valid test zones"
        
        # Estimate impact of moving to lowest suggested zone
        lowest_zone = min(suggested_zones)
        zone_change = current_optimal - lowest_zone
        
        if abs(elasticity) > 0.05:  # Meaningful elasticity
            estimated_impact = zone_change * elasticity * 100
            if estimated_impact > 0:
                return f"Potential {estimated_impact:.0f}% volume increase"
            else:
                return f"Risk of {abs(estimated_impact):.0f}% volume decrease"
        else:
            return "Low price sensitivity - modest impact expected"
    
    def _estimate_implementation_timeline(self, needs: Dict) -> str:
        """Estimate timeline for implementing recommendations"""
        
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            return "8-12 weeks (requires zone structure changes)"
        elif needs['type'] in ['TEST_LOWER_ZONES', 'TEST_HIGHER_ZONES']:
            return "2-4 weeks (simple zone adjustments)"
        elif needs['type'] == 'HIGH_SENSITIVITY':
            return "Ongoing monitoring (no immediate action)"
        else:
            return "4-6 weeks"
    
    def _assess_zone_change_risk(self, analysis: Dict, needs: Dict) -> Dict:
        """Assess risk of implementing zone changes"""
        
        risk_factors = []
        risk_level = "LOW"
        
        volume = analysis['total_volume']
        elasticity = abs(analysis['price_elasticity'])
        
        # High volume at stake
        if volume > 10000:
            risk_factors.append(f"High volume at stake: ${volume:,.0f}")
            risk_level = "MEDIUM"
        
        # High price sensitivity
        if elasticity > 0.2:
            risk_factors.append("High price sensitivity - changes will have major impact")
            risk_level = "MEDIUM"
        
        # Granularity expansion risks
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            risk_factors.append("Requires significant zone structure changes")
            risk_level = "HIGH"
        
        # Limited testing data
        zones_tested = len(analysis['current_zones_tested'])
        if zones_tested < 3:
            risk_factors.append(f"Limited zone testing data ({zones_tested} zones)")
            if risk_level == "LOW":
                risk_level = "MEDIUM"
        
        return {
            'risk_level': risk_level,
            'risk_factors': risk_factors,
            'mitigation_strategies': self._suggest_risk_mitigation(risk_factors, needs)
        }
    
    def _suggest_risk_mitigation(self, risk_factors: List[str], needs: Dict) -> List[str]:
        """Suggest risk mitigation strategies"""
        
        mitigations = []
        
        if any("High volume" in factor for factor in risk_factors):
            mitigations.append("Phase implementation: start with 25% of volume for 2 weeks")
        
        if any("High price sensitivity" in factor for factor in risk_factors):
            mitigations.append("Monitor daily for first week, weekly thereafter")
        
        if any("zone structure changes" in factor for factor in risk_factors):
            mitigations.append("Coordinate with pricing team, IT, and operations before implementation")
        
        if any("Limited zone testing" in factor for factor in risk_factors):
            mitigations.append("Start with most conservative zone recommendation first")
        
        if needs['type'] == 'GRANULARITY_EXPANSION_NEEDED':
            mitigations.append("Test zone 0.8 first as safest starting point near zone 1")
        
        return mitigations
    
    def predict_volume_turnaround_timeline(self, recommendations: List[Dict], 
                                         current_trend: Dict) -> Dict:
        """Predict when volume will turn positive based on zone optimization recommendations"""
        
        # Current situation
        current_decline_rate = current_trend.get('volume_decline_rate_pct', 0)
        total_volume = current_trend.get('total_volume', 0)
        
        if total_volume == 0:
            return {'error': 'No volume data for prediction'}
        
        # Analyze recommendations by implementation timeline
        immediate_impact_recs = [r for r in recommendations if '2-4 weeks' in r.get('expected_timeline', '')]
        medium_term_recs = [r for r in recommendations if '4-6 weeks' in r.get('expected_timeline', '')]
        long_term_recs = [r for r in recommendations if '8-12 weeks' in r.get('expected_timeline', '')]
        
        # Estimate volume impact from each wave
        immediate_volume_lift = sum(self._extract_volume_impact(r) for r in immediate_impact_recs)
        medium_volume_lift = sum(self._extract_volume_impact(r) for r in medium_term_recs)
        long_volume_lift = sum(self._extract_volume_impact(r) for r in long_term_recs)
        
        total_potential_lift = immediate_volume_lift + medium_volume_lift + long_volume_lift
        
        # Calculate turnaround timeline
        if total_potential_lift > abs(current_decline_rate * total_volume):
            # We can overcome the decline
            if immediate_volume_lift > abs(current_decline_rate * total_volume * 0.7):
                turnaround_week = 6  # Fast turnaround
            elif immediate_volume_lift + medium_volume_lift > abs(current_decline_rate * total_volume):
                turnaround_week = 10  # Medium turnaround
            else:
                turnaround_week = 16  # Longer turnaround
        else:
            turnaround_week = None  # Need more aggressive action
        
        return {
            'current_decline_rate_pct': current_decline_rate,
            'total_volume_base': total_volume,
            'potential_volume_lift': total_potential_lift,
            'immediate_impact_volume': immediate_volume_lift,
            'medium_term_impact_volume': medium_volume_lift,
            'long_term_impact_volume': long_volume_lift,
            'predicted_turnaround_week': turnaround_week,
            'confidence': self._calculate_prediction_confidence(recommendations, total_potential_lift),
            'key_dependencies': [
                f"{len(immediate_impact_recs)} immediate-impact zone changes",
                f"{len(long_term_recs)} granularity expansions", 
                "Successful implementation without major operational issues"
            ]
        }
    
    def _extract_volume_impact(self, recommendation: Dict) -> float:
        """Extract estimated volume impact from a recommendation"""
        
        volume_at_stake = recommendation.get('total_volume_at_stake', 0)
        
        # Parse expected impact
        impact_text = recommendation.get('expected_volume_impact', '0')
        if 'increase' in str(impact_text).lower():
            # Extract percentage
            import re
            pct_match = re.search(r'(\d+)%', str(impact_text))
            if pct_match:
                pct_increase = float(pct_match.group(1)) / 100
                return volume_at_stake * pct_increase
        
        # Default conservative estimate based on recommendation type
        if recommendation['recommendation_type'] == 'GRANULARITY_EXPANSION_NEEDED':
            return volume_at_stake * 0.2  # 20% conservative estimate
        elif recommendation['recommendation_type'] == 'TEST_LOWER_ZONES':
            return volume_at_stake * 0.1  # 10% conservative estimate
        
        return 0
    
    def _calculate_prediction_confidence(self, recommendations: List[Dict], 
                                       total_lift: float) -> float:
        """Calculate confidence in the turnaround prediction"""
        
        confidence = 0.5  # Start at 50%
        
        # More recommendations = higher confidence (up to a point)
        rec_confidence = min(len(recommendations) / 10.0, 0.3)
        confidence += rec_confidence
        
        # Higher volume impact = higher confidence 
        if total_lift > 0:
            impact_confidence = min(total_lift / 50000, 0.2)  # Cap at 20%
            confidence += impact_confidence
        
        return min(confidence, 0.9)  # Cap at 90%
    
    def generate_executive_dashboard(self, recommendations: List[Dict], turnaround_prediction: Dict) -> Dict:
        """
        Generate executive dashboard for zone optimization
        (compatible with both standard and historical-proven recs)
        """

        # Treat “high priority” as either high score OR high confidence historical
        high_priority = [
            r for r in recommendations
            if r.get('implementation_priority', 0) >= 70 or r.get('confidence', 0) >= 0.8
        ]

        # Count granularity from either standard need OR historical proven pattern that implies it
        granularity_expansions = [
            r for r in recommendations
            if r.get('recommendation_type') == 'GRANULARITY_EXPANSION_NEEDED'
            or r.get('proven_pattern') == 'GRANULARITY_EXPANSION_VALIDATED'
        ]

        # “Zone tests recommended” = any explicit TEST_* type OR historical-proven direct zone-change
        zone_tests = [
            r for r in recommendations
            if (r.get('recommendation_type', '').startswith('TEST_'))
            or (r.get('recommendation_type') == 'HISTORICAL_PROVEN')
        ]

        # Coerce to plain float to avoid np.float64(...) in printouts
        total_volume_opportunity = float(sum(r.get('total_volume_at_stake', 0) or 0 for r in recommendations))

        exec_summary = {
            'total_zone_opportunities': len(recommendations),
            'high_priority_actions': len(high_priority),
            'granularity_expansions_needed': len(granularity_expansions),
            'zone_tests_recommended': len(zone_tests),
            'total_volume_in_scope': total_volume_opportunity,
            'predicted_turnaround_week': turnaround_prediction.get('predicted_turnaround_week', 'TBD')
        }

        return {
            'executive_summary': exec_summary,
            'immediate_action_items': [
                {
                    'company': r.get('company_name'),
                    'combo': r.get('combo_description'),
                    'action': (r.get('specific_actions') or ['Review needed'])[0],
                    'volume': r.get('total_volume_at_stake', 0),
                    'priority': r.get('implementation_priority', 0)
                }
                for r in sorted(high_priority, key=lambda x: x.get('implementation_priority', 0), reverse=True)[:5]
            ],
            'granularity_expansion_alerts': [
                {
                    'company': r.get('company_name'),
                    'combo': r.get('combo_description'),
                    'issue': 'ZONE 0 RECOMMENDED / HISTORICAL TILT TO ≤1 — CONSIDER FRACTIONAL ZONES',
                    'volume': r.get('total_volume_at_stake', 0)
                }
                for r in granularity_expansions
            ],
            'timeline_forecast': turnaround_prediction,
            'success_metrics': {
                'volume_growth_target': f"+{turnaround_prediction.get('potential_volume_lift', 0):,.0f} lbs",
                'customer_growth_target': "Est. +10–15% distinct customers",
                'margin_impact': "Expected margin compression offset by volume gains"
            }
        }

    def predict_historical_turnaround_timeline(self, recommendations: List[Dict], current_trend: Dict) -> Dict:
        current_decline_rate = current_trend.get('volume_decline_rate_pct', 0)
        total_volume = current_trend.get('total_volume', 0)
        if total_volume == 0:
            return {'error': 'No volume data for prediction'}

        high_confidence_recs = [r for r in recommendations if r.get('confidence', 0) >= 0.8]
        medium_confidence_recs = [r for r in recommendations if 0.6 <= r.get('confidence', 0) < 0.8]

        high_conf_volume_lift = sum(self._extract_historical_volume_impact(r) for r in high_confidence_recs)
        medium_conf_volume_lift = sum(self._extract_historical_volume_impact(r) for r in medium_confidence_recs)
        total_potential_lift = high_conf_volume_lift + medium_conf_volume_lift

        if high_conf_volume_lift > abs(current_decline_rate * total_volume * 0.8):
            turnaround_week = 3
        elif total_potential_lift > abs(current_decline_rate * total_volume):
            turnaround_week = 5
        else:
            turnaround_week = 6

        # IMPORTANT: include the keys your Excel writer expects
        return {
            'current_decline_rate_pct': current_decline_rate,
            'total_volume_base': total_volume,
            'potential_volume_lift': total_potential_lift,
            'immediate_impact_volume': high_conf_volume_lift,   # alias for Excel
            'medium_term_impact_volume': medium_conf_volume_lift,  # alias for Excel
            'long_term_impact_volume': 0,  # no “long” bucket here; set 0
            'predicted_turnaround_week': turnaround_week,
            'confidence': min(len(high_confidence_recs) / 5.0, 0.95),
            'key_dependencies': [
                f"{len(high_confidence_recs)} historically-proven zone changes",
                "2+ years of historical validation data",
                "Immediate implementation possible"
            ],
            'six_week_feasibility': 'ACHIEVABLE' if turnaround_week <= 6 else 'REQUIRES_MORE_AGGRESSIVE_ACTION'
        }

    def _extract_historical_volume_impact(self, recommendation: Dict) -> float:
        volume_at_stake = recommendation.get('total_volume_at_stake', 0)
        confidence = recommendation.get('confidence', 0.5)
        if recommendation.get('type') == 'HISTORICAL_PROVEN':
            if confidence >= 0.9:   return volume_at_stake * 0.25
            if confidence >= 0.8:   return volume_at_stake * 0.20
            if confidence >= 0.7:   return volume_at_stake * 0.15
        return volume_at_stake * 0.10

    def save_learning_state(self, filepath: str):
        """Save the current learning state for persistence"""
        
        state = {
            'zone_performance': dict(self.zone_performance),
            'zone_change_outcomes': list(self.zone_change_outcomes),
            'granularity_needs': dict(self.granularity_needs),
            'price_sensitivity_patterns': dict(self.price_sensitivity_patterns),
            'saved_at': datetime.now().isoformat()
        }
        
        with open(filepath, 'w') as f:
            json.dump(state, f, indent=2, default=str)
    
    def load_learning_state(self, filepath: str):
        """Load previously saved learning state"""
        
        if os.path.exists(filepath):
            with open(filepath, 'r') as f:
                state = json.load(f)
            
            self.zone_performance = defaultdict(lambda: {
                'volume_elasticity': 0.0,
                'customer_elasticity': 0.0,
                'optimal_zone_estimate': 5.0,
                'zone_effectiveness_curve': {},
                'sample_count': 0,
                'confidence': 0.0,
                'last_updated': datetime.now(),
                'historical_performance': []
            }, state.get('zone_performance', {}))
            
            self.zone_change_outcomes = deque(state.get('zone_change_outcomes', []), maxlen=500)
            self.granularity_needs = defaultdict(lambda: {
                'needs_expansion': False,
                'recommended_zones_between_0_1': 0,
                'zero_recommendations_count': 0,
                'evidence_score': 0.0
            }, state.get('granularity_needs', {}))
            self.price_sensitivity_patterns = defaultdict(list, state.get('price_sensitivity_patterns', {}))
            

def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:
    if df is None:
        return None

    ren = {
        'Pounds CY': 'Pounds_CY',
        'Pounds PY': 'Pounds_PY',
        'Delta Pounds YoY': 'Delta_YoY_Lbs',
        'Zone Suffix': 'Zone_Suffix',
        'Fiscal Week Number': 'Fiscal Week Number',  # keep name consistent
        'Company Customer Number': 'Distinct_Customers_CY',  # best available proxy
    }

    out = df.copy()

    # 1) rename
    for k, v in ren.items():
        if k in out.columns and v not in out.columns:
            out = out.rename(columns={k: v})

    # 2) coerce numeric columns used downstream
    num_cols = [
        'Pounds_CY', 'Pounds_PY', 'Delta_YoY_Lbs',
        'Distinct_Customers_CY', 'Distinct_Customers_PY',
        'Zone_Suffix_Numeric', 'Fiscal Week Number'
    ]
    for c in num_cols:
        if c in out.columns:
            out[c] = pd.to_numeric(out[c], errors='coerce')

    # 3) fill sensible defaults
    if 'Distinct_Customers_CY' in out.columns and 'Distinct_Customers_PY' not in out.columns:
        out['Distinct_Customers_PY'] = out['Distinct_Customers_CY']

    # 4) zeros for NaNs in key numeric fields
    for c in ['Pounds_CY', 'Pounds_PY', 'Delta_YoY_Lbs', 'Distinct_Customers_CY', 'Distinct_Customers_PY']:
        if c in out.columns:
            out[c] = out[c].fillna(0)

    return out

from typing import Dict, Any, Optional

def integrate_zone_optimization_with_historical_data(
    status_df: pd.DataFrame,
    raw_df: pd.DataFrame,
    historical_df: Optional[pd.DataFrame] = None
) -> Dict[str, Any]:
    """
    Runs the full zone optimization flow using current (status_df), raw_df,
    and optional historical_df. DMA is NOT used in keys. Business Center is NOT used.
    CPA-only + no exceptions/discounts are enforced via _apply_scope_filters.
    """
    # 1) Create engine
    engine = ZoneOptimizationEngine(historical_analysis_mode=True)

    # 2) Apply scope filters (free function, not self.*)
    status_df = _apply_scope_filters(status_df)
    raw_df    = _apply_scope_filters(raw_df)
    if historical_df is not None:
        historical_df = _apply_scope_filters(historical_df)

    # 3) Feature extraction & analysis
    zone_df = engine.extract_zone_features(status_df)
    zone_analysis = engine.analyze_zone_effectiveness(zone_df)

    # 4) Recommendations + timeline prediction
    if historical_df is not None and len(historical_df) > 0:
        print("Historical data provided - analyzing patterns...")
        engine.analyze_historical_zone_changes(historical_df)  # learn patterns
        recommendations = engine.generate_immediate_high_confidence_recommendations(status_df)
        current_trend = {
            'volume_decline_rate_pct': float(zone_df['Volume_Growth_Rate'].mean()) if 'Volume_Growth_Rate' in zone_df else 0.0,
            'total_volume': float(zone_df['Pounds_CY'].sum()) if 'Pounds_CY' in zone_df else 0.0,
        }
        turnaround_prediction = engine.predict_historical_turnaround_timeline(recommendations, current_trend)
    else:
        print("No historical data - using current analysis only...")
        recommendations = engine.generate_zone_recommendations(zone_analysis)
        current_trend = {
            'volume_decline_rate_pct': float(zone_df['Volume_Growth_Rate'].mean()) if 'Volume_Growth_Rate' in zone_df else 0.0,
            'total_volume': float(zone_df['Pounds_CY'].sum()) if 'Pounds_CY' in zone_df else 0.0,
        }
        turnaround_prediction = engine.predict_volume_turnaround_timeline(recommendations, current_trend)

    # 5) Executive dashboard
    executive_dashboard = engine.generate_executive_dashboard(recommendations, turnaround_prediction)

    # 6) Return payload
    zone_assignments = zone_df['Company_Combo_Key'].tolist() if 'Company_Combo_Key' in zone_df.columns else []
    return {
        'zone_analysis': zone_analysis,
        'recommendations': recommendations,
        'turnaround_prediction': turnaround_prediction,
        'executive_dashboard': executive_dashboard,
        'historical_analysis_used': (historical_df is not None and len(historical_df) > 0),
        'historical_patterns_count': len(getattr(engine, 'historical_patterns', {})),
        'zone_assignments': zone_assignments,
    }

# Function to update your existing Excel writer
def add_zone_optimization_to_excel(xw, zone_optimization_results):
    """
    Add zone optimization tabs to your existing Excel file
    Call this function from within your write_excel function
    """
    from openpyxl.styles import Font

    if not zone_optimization_results:
        return

    dashboard = zone_optimization_results['executive_dashboard']

    # 10 - Zone Dashboard
    exec_df = pd.DataFrame([dashboard['executive_summary']])
    exec_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=0)

    ws = xw.book["10_Zone_Dashboard"]
    next_row = exec_df.shape[0] + 3  # safe starting row regardless of items

    # Immediate action items (optional)
    if dashboard['immediate_action_items']:
        ws.cell(row=next_row, column=1, value="IMMEDIATE ACTION ITEMS - TOP PRIORITIES")
        ws.cell(row=next_row, column=1).font = Font(bold=True)
        actions_df = pd.DataFrame(dashboard['immediate_action_items'])
        actions_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=next_row + 1)
        next_row = next_row + 1 + actions_df.shape[0] + 3  # advance pointer

    # Granularity expansion alerts (optional)
    if dashboard['granularity_expansion_alerts']:
        ws.cell(row=next_row, column=1, value="⚠️ ZONE 0 ALERTS - GRANULARITY EXPANSION NEEDED")
        ws.cell(row=next_row, column=1).font = Font(bold=True, color="FF0000")
        alerts_df = pd.DataFrame(dashboard['granularity_expansion_alerts'])
        alerts_df.to_excel(xw, sheet_name="10_Zone_Dashboard", index=False, startrow=next_row + 1)
        next_row = next_row + 1 + alerts_df.shape[0] + 3

    # 11 - Detailed Zone Recommendations
    recs_df = pd.DataFrame(zone_optimization_results['recommendations'])
# After: recs_df = pd.DataFrame(zone_optimization_results['recommendations'])
    if not recs_df.empty:
        # Flatten nested dicts
        for side in ("behavior_current", "behavior_recommended"):
            if side in recs_df.columns:
                flat = pd.json_normalize(recs_df[side]).add_prefix(f"{side}.")
                recs_df = pd.concat([recs_df.drop(columns=[side]), flat], axis=1)

        # Ensure expected columns exist for Excel logic
        needed = [
            "behavior_current.repeat_buyer_rate","behavior_current.retention_4w","behavior_current.active_buyer_share",
            "behavior_recommended.repeat_buyer_rate","behavior_recommended.retention_4w","behavior_recommended.active_buyer_share",
            "behavior_current.BehaviorScore","behavior_recommended.BehaviorScore"
        ]
        for c in needed:
            if c not in recs_df.columns: recs_df[c] = None

        recs_df.to_excel(xw, sheet_name="11_Zone_Recommendations", index=False)

    # After writing 11_Zone_Recommendations
    if "11_Zone_Recommendations" in xw.book.sheetnames:
        ws = xw.book["11_Zone_Recommendations"]

        # Add a boolean column: Behavior_NonInferior (true if our guardrail passed)
        cur_rb = _col_index_by_header(ws, "behavior_current.repeat_buyer_rate")
        rec_rb = _col_index_by_header(ws, "behavior_recommended.repeat_buyer_rate")
        cur_rt = _col_index_by_header(ws, "behavior_current.retention_4w")
        rec_rt = _col_index_by_header(ws, "behavior_recommended.retention_4w")
        cur_ab = _col_index_by_header(ws, "behavior_current.active_buyer_share")
        rec_ab = _col_index_by_header(ws, "behavior_recommended.active_buyer_share")

        if min(cur_rb, rec_rb, cur_rt, rec_rt, cur_ab, rec_ab) > 0:
            new_c = ws.max_column + 1
            ws.cell(1, new_c, "Behavior_NonInferior?")
            eps = ALLOW_UPZONE_EPSILON
            for r in range(2, ws.max_row + 1):
                ws.cell(
                    r, new_c,
                    f"=AND("
                    f"{get_column_letter(rec_rb)}{r}>={get_column_letter(cur_rb)}{r}+{eps},"
                    f"{get_column_letter(rec_rt)}{r}>={get_column_letter(cur_rt)}{r}+{eps},"
                    f"{get_column_letter(rec_ab)}{r}>={get_column_letter(cur_ab)}{r}+{eps})"
                )
            # Shade TRUE green, FALSE red
            rng = f"{get_column_letter(new_c)}2:{get_column_letter(new_c)}{ws.max_row}"
            ws.conditional_formatting.add(rng, CellIsRule(operator='equal', formula=['TRUE'], fill=None, font=None, stopIfTrue=False))
            ws.conditional_formatting.add(rng, ColorScaleRule(start_type='min', start_color='FFC7CE', end_type='max', end_color='C6EFCE'))

    # --- Conditional formatting: 11b_Behavior_Compare ---
    if "11b_Behavior_Compare" in xw.book.sheetnames:
        ws = xw.book["11b_Behavior_Compare"]

        # Build (or find) Delta columns
        deltas = []
        for metric in ["Repeat_Buyer_Rate", "Retention_4w", "Active_Buyer_Share", "BehaviorScore"]:
            new_c = _add_delta_column(
                ws,
                f"Current_{metric}",
                f"Recommended_{metric}",
                f"Δ_{metric}"
            )
            if new_c > 0:
                deltas.append(new_c)

        # 3-color scale on each Δ_ column (red=low, yellow=mid, green=high)
        for c in deltas:
            col_letter = get_column_letter(c)
            rng = f"{col_letter}2:{col_letter}{ws.max_row}"
            ws.conditional_formatting.add(
                rng,
                ColorScaleRule(
                    start_type='num', start_value=-0.05, start_color='FFC7CE',  # red
                    mid_type='num', mid_value=0.0,  mid_color='FFEB9C',        # yellow
                    end_type='num', end_value=0.05, end_color='C6EFCE'         # green
                )
            )

        # Data bars on Recommended_BehaviorScore so high scores stand out
        rec_bs_c = _col_index_by_header(ws, "Recommended_BehaviorScore")
        if rec_bs_c > 0:
            rng = f"{get_column_letter(rec_bs_c)}2:{get_column_letter(rec_bs_c)}{ws.max_row}"
            ws.conditional_formatting.add(rng, DataBarRule(showValue=True))
            # For the row-bold “winner” highlight:
            dxf_bold = DifferentialStyle(font=Font(bold=True))
            ws.conditional_formatting.add(
                f"A{r}:{last_col_letter}{r}",
                FormulaRule(formula=[f"${get_column_letter(znum_c)}{r}=${get_column_letter(optz_c)}{r}"], dxf=dxf_bold)
            )

    # 12 - Zone Performance Analysis
    zone_analysis = zone_optimization_results['zone_analysis']
    if zone_analysis:
        analysis_rows = []
        for company_combo, analysis in zone_analysis.items():
            base_row = {
                'Company_Combo': company_combo,
                'Company_Name': analysis['company_name'],
                'Combo_Description': analysis['combo_key'],
                'Current_Optimal_Zone': analysis['optimal_zone'],
                'Optimal_Performance_Score': analysis['optimal_performance'],  # now BehaviorScore if you patched earlier
                'Price_Elasticity': analysis['price_elasticity'],
                'Total_Volume': analysis['total_volume'],
                'Zones_Tested': ', '.join(map(str, analysis['current_zones_tested'])),
                'Analysis_Type': analysis['needs_analysis']['type'],
                'Confidence': analysis['needs_analysis']['confidence'],
                'Priority': analysis['needs_analysis']['priority']
            }

            for zone_perf in analysis['zone_performance_curve']:
                row = base_row.copy()
                # Existing fields
                row['Zone_Number'] = zone_perf.get('Zone_Suffix_Numeric')
                row['Zone_Volume_Performance'] = zone_perf.get('Volume_Performance')
                row['Zone_Volume_Growth_Rate'] = zone_perf.get('Volume_Growth_Rate')
                row['Zone_Customer_Growth_Rate'] = zone_perf.get('Customer_Growth_Rate')
                row['Zone_Combined_Score'] = zone_perf.get('Combined_Performance_Score')

                # === NEW behavior metrics (safe .get calls) ===
                row['Zone_Repeat_Buyer_Rate']   = zone_perf.get('repeat_buyer_rate')
                row['Zone_Retention_4w']        = zone_perf.get('retention_4w')
                row['Zone_Active_Buyer_Share']  = zone_perf.get('active_buyer_share')
                row['Zone_Avg_Freq']            = zone_perf.get('avg_freq')
                row['Zone_Avg_Recency']         = zone_perf.get('avg_recency')
                row['Zone_Avg_Margin_per_LB']   = zone_perf.get('avg_margin_per_lb')
                row['Zone_BehaviorScore']       = zone_perf.get('BehaviorScore')

                analysis_rows.append(row)

        if analysis_rows:
            perf_df = pd.DataFrame(analysis_rows)

            # Make sure columns exist even if some combos lack behavior data
            must_have = [
                'Zone_Repeat_Buyer_Rate','Zone_Retention_4w','Zone_Active_Buyer_Share',
                'Zone_Avg_Freq','Zone_Avg_Recency','Zone_Avg_Margin_per_LB','Zone_BehaviorScore'
            ]
            for c in must_have:
                if c not in perf_df.columns:
                    perf_df[c] = None

            perf_df.to_excel(xw, sheet_name="12_Zone_Performance", index=False)
    # --- Conditional formatting: 12_Zone_Performance ---
    if "12_Zone_Performance" in xw.book.sheetnames:
        ws = xw.book["12_Zone_Performance"]

        # Data bars on BehaviorScore and Volume to see curve shape quickly
        for header in ["Zone_BehaviorScore", "Total_Volume", "Zone_Combined_Score"]:
            c = _col_index_by_header(ws, header)
            if c > 0:
                rng = f"{get_column_letter(c)}2:{get_column_letter(c)}{ws.max_row}"
                ws.conditional_formatting.add(rng, DataBarRule(showValue=True))
                # For the row-bold “winner” highlight:
                dxf_bold = DifferentialStyle(font=Font(bold=True))
                ws.conditional_formatting.add(
                    f"A{r}:{last_col_letter}{r}",
                    FormulaRule(formula=[f"${get_column_letter(znum_c)}{r}=${get_column_letter(optz_c)}{r}"], dxf=dxf_bold)
                )
                
        # 3-color scale on Zone_Repeat_Buyer_Rate and Zone_Retention_4w (red→yellow→green)
        for header in ["Zone_Repeat_Buyer_Rate", "Zone_Retention_4w"]:
            c = _col_index_by_header(ws, header)
            if c > 0:
                rng = f"{get_column_letter(c)}2:{get_column_letter(c)}{ws.max_row}"
                ws.conditional_formatting.add(
                    rng,
                    ColorScaleRule(
                        start_type='num', start_value=0.0, start_color='FFC7CE',
                        mid_type='num', mid_value=0.5, mid_color='FFEB9C',
                        end_type='num', end_value=1.0, end_color='C6EFCE'
                    )
                )

        # Bold the rows where Zone_Number == Current_Optimal_Zone (visual “winner”)
        znum_c = _col_index_by_header(ws, "Zone_Number")
        optz_c = _col_index_by_header(ws, "Current_Optimal_Zone")
        if znum_c > 0 and optz_c > 0:
            # Apply a formula rule across the whole row range
            last_col_letter = get_column_letter(ws.max_column)
            for r in range(2, ws.max_row + 1):
                ws.conditional_formatting.add(
                    f"A{r}:{last_col_letter}{r}",
                    FormulaRule(
                        formula=[f"${get_column_letter(znum_c)}{r}=${get_column_letter(optz_c)}{r}"],
                        stopIfTrue=False,
                        font={'bold': True}
                    )
                )

    # 13 - Timeline Prediction
    timeline = zone_optimization_results['turnaround_prediction']
    if timeline and 'error' not in timeline:
        timeline_df = pd.DataFrame([{
            'Current_Decline_Rate_Pct': timeline.get('current_decline_rate_pct', 0) * 100,
            'Total_Volume_Base': timeline.get('total_volume_base', timeline.get('total_volume', 0)),
            'Potential_Volume_Lift': timeline.get('potential_volume_lift', 0),
            'Immediate_Impact_Volume': timeline.get('immediate_impact_volume', 0),
            'Medium_Term_Impact_Volume': timeline.get('medium_term_impact_volume', 0),
            'Long_Term_Impact_Volume': timeline.get('long_term_impact_volume', 0),
            'Predicted_Turnaround_Week': timeline.get('predicted_turnaround_week'),
            'Prediction_Confidence': timeline.get('confidence', 0) * 100
        }])
        timeline_df.to_excel(xw, sheet_name="13_Timeline_Prediction", index=False)

        if 'key_dependencies' in timeline:
            deps_df = pd.DataFrame(timeline['key_dependencies'], columns=['Key_Dependencies'])
            deps_start = timeline_df.shape[0] + 3
            deps_df.to_excel(xw, sheet_name="13_Timeline_Prediction", index=False, startrow=deps_start)


def create_enhanced_email_body(zone_optimization_results, source_file_name: str) -> str:
    base_body = f"Auto-generated report for {source_file_name} at {datetime.now():%Y-%m-%d %H:%M}."
    if not zone_optimization_results:
        return base_body
    zone_summary = zone_optimization_results['executive_dashboard']['executive_summary']
    enhanced_body = f"""{base_body}

ZONE OPTIMIZATION INSIGHTS:
• {zone_summary['total_zone_opportunities']} zone optimization opportunities identified
• {zone_summary['high_priority_actions']} high-priority actions recommended
• {zone_summary['granularity_expansions_needed']} combos need granularity expansion (flagged Zone 0)
• Predicted volume turnaround: Week {zone_summary['predicted_turnaround_week']}

Key Actions Needed:
""" + "\n".join(
        f"• {action['company']}: {action['action']}"
        for action in zone_optimization_results['executive_dashboard']['immediate_action_items'][:3]
    )
    return enhanced_body


def add_zone_optimization_summary(summary_kpi_df: pd.DataFrame, zone_optimization_results: dict) -> pd.DataFrame:
    if not zone_optimization_results:
        return summary_kpi_df
    exec_summary = zone_optimization_results['executive_dashboard']['executive_summary']
    zone_summary_row = pd.DataFrame([{
        'KPI': 'Zone Optimization Summary',
        'Value': f"{exec_summary['total_zone_opportunities']} opportunities",
        'High_Priority_Actions': exec_summary['high_priority_actions'],
        'Granularity_Expansions_Needed': exec_summary['granularity_expansions_needed'],
        'Predicted_Turnaround_Week': exec_summary['predicted_turnaround_week']
    }])
    return pd.concat([summary_kpi_df, zone_summary_row], ignore_index=True)


if __name__ == "__main__":
    # 0) Historical file paths
    hist_paths = [
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_1.csv",
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_2.csv",
    ]

    # 1) Load & combine historicals
    hist_list = []
    for p in hist_paths:
        if os.path.exists(p):
            df = pd.read_csv(p, low_memory=False)
            hist_list.append(df)
            print(f"Loaded {os.path.basename(p)}: {len(df):,} rows")
        else:
            print(f"WARNING: not found -> {p}")

    if not hist_list:
        raise FileNotFoundError("No historical files loaded. Check paths.")

    historical = pd.concat(hist_list, ignore_index=True)
    historical = _normalize_cols(historical)

    # 2) Ensure week is numeric and present
    if 'Fiscal Week Number' not in historical.columns:
        raise KeyError("Historical files must include 'Fiscal Week Number'.")
    historical['Fiscal Week Number'] = pd.to_numeric(
        historical['Fiscal Week Number'], errors='coerce'
    )
    historical = historical.dropna(subset=['Fiscal Week Number'])
    historical['Fiscal Week Number'] = historical['Fiscal Week Number'].astype(int)

    # 3) Build CURRENT snapshot from latest available week in historicals
    latest_week = historical['Fiscal Week Number'].max()
    print(f"Using latest fiscal week from historicals as CURRENT: {latest_week}")
    latest = historical[historical['Fiscal Week Number'] == latest_week].copy()
    latest = _normalize_cols(latest)  # <-- important to avoid str vs int errors

    # 4) Aggregate CURRENT snapshot to combo level
    group_cols = ['Company Name', 'NPD Cuisine Type', 'Attribute Group ID', 'Price Source Type']
    for gc in group_cols:
        if gc not in latest.columns:
            latest[gc] = "UNKNOWN"

    # build aggregation map with normalized names
    agg_map = {}
    if 'Pounds_CY' in latest.columns:     agg_map['Pounds_CY'] = 'sum'
    if 'Pounds_PY' in latest.columns:     agg_map['Pounds_PY'] = 'sum'
    if 'Delta_YoY_Lbs' in latest.columns: agg_map['Delta_YoY_Lbs'] = 'sum'

    # distinct customers (prefer an id column if present)
    cust_id_col = None
    for cand in ['Company Customer Number', 'Customer Number', 'Customer ID']:
        if cand in latest.columns:
            cust_id_col = cand
            break
    if cust_id_col:
        agg_map[cust_id_col] = 'nunique'

    current_aggregated = latest.groupby(group_cols, dropna=False).agg(agg_map).reset_index()
    current_aggregated = _normalize_cols(current_aggregated)

    # 5) Bring representative Zone & Week back to the aggregated frame
    zref_cols = [c for c in ['Price Zone ID', 'Zone_Suffix', 'Fiscal Week Number'] if c in latest.columns]
    zref = latest[group_cols + zref_cols].copy()

    if 'Price Zone ID' in zref.columns:
        zref = zref.groupby(group_cols, dropna=False).agg({
            'Price Zone ID': lambda s: s.mode().iat[0] if not s.mode().empty
                                      else (s.dropna().iat[0] if s.dropna().size else None),
            'Fiscal Week Number': 'max' if 'Fiscal Week Number' in zref.columns else 'max'
        }).reset_index()
    else:
        # fallback: use Zone_Suffix and synthesize a Price Zone ID
        zref = zref.groupby(group_cols, dropna=False).agg({
            'Zone_Suffix': lambda s: s.mode().iat[0] if not s.mode().empty
                                     else (s.dropna().iat[0] if s.dropna().size else None),
            'Fiscal Week Number': 'max' if 'Fiscal Week Number' in zref.columns else 'max'
        }).reset_index()
        if 'Zone_Suffix' in zref.columns:
            zref['Price Zone ID'] = zref['Zone_Suffix'].astype(str)

    current_aggregated = current_aggregated.merge(
        zref[[*group_cols, 'Price Zone ID', 'Fiscal Week Number']].drop_duplicates(),
        on=group_cols, how='left'
    )

    # 6) Run the engine (learns from full historical, recommends on current)
    results = integrate_zone_optimization_with_historical_data(
        status_df=current_aggregated,
        raw_df=latest,
        historical_df=historical
    )

    print("Executive summary:", results['executive_dashboard']['executive_summary'])
    
        # --- SAVE ARTIFACTS ---
    # 1) CSV of recommendations
    pd.DataFrame(results['recommendations']).to_csv(
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\zone_recommendations.csv",
        index=False
    )

    # 2) CSV of detailed zone analysis
    za = results['zone_analysis']
    if isinstance(za, dict) and za:
        rows = []
        for k, a in za.items():
            base = {
                'Company_Combo': k,
                'Company_Name': a['company_name'],
                'Combo_Description': a['combo_key'],
                'Current_Optimal_Zone': a['optimal_zone'],
                'Optimal_Performance_Score': a['optimal_performance'],
                'Price_Elasticity': a['price_elasticity'],
                'Total_Volume': a['total_volume'],
                'Zones_Tested': ', '.join(map(str, a['current_zones_tested'])),
                'Analysis_Type': a['needs_analysis']['type'],
                'Confidence': a['needs_analysis']['confidence'],
                'Priority': a['needs_analysis']['priority'],
            }
            for zp in a['zone_performance_curve']:
                row = base.copy()
                row.update({
                    'Zone_Number': zp['Zone_Suffix_Numeric'],
                    'Zone_Volume_Performance': zp['Volume_Performance'],
                    'Zone_Volume_Growth_Rate': zp['Volume_Growth_Rate'],
                    'Zone_Customer_Growth_Rate': zp['Customer_Growth_Rate'],
                    'Zone_Combined_Score': zp['Combined_Performance_Score'],
                })
                rows.append(row)
        pd.DataFrame(rows).to_csv(
            r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\zone_performance_detail.csv",
            index=False
        )

    # 3) Excel dashboard tabs
    from openpyxl import Workbook
    from pandas import ExcelWriter
    with ExcelWriter(
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\zone_dashboard.xlsx",
        engine="openpyxl"
    ) as xw:
        add_zone_optimization_to_excel(xw, results)
    print("Artifacts saved: CSVs + Excel dashboard.")

