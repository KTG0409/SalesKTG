#!/usr/bin/env python3
"""
Foodservice Zone Optimization Engine v9
NOW WITH: Flexible column configuration + Learning system
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Set
from collections import defaultdict
import warnings
import os
import json
from dataclasses import dataclass, asdict
warnings.filterwarnings('ignore')

from openpyxl.utils import get_column_letter
from openpyxl.styles import Alignment, Font, PatternFill, Border, Side
from openpyxl.formatting.rule import ColorScaleRule, CellIsRule, DataBarRule, Rule
from openpyxl.styles.differential import DifferentialStyle

# ============================================================================
# ENHANCED CONFIGURATION
# ============================================================================

@dataclass
@dataclass
class DataConfiguration:
    """Configure which columns to use for grouping."""
    
    # Required columns (always needed)
    company_column: str = 'Company Name'
    customer_id_column: str = 'Company Customer Number'
    last_invoice_date_column: str = 'Last Invoice Date'
    fiscal_week_column: str = 'Fiscal Week Number'
    pounds_cy_column: str = 'Pounds_CY'
    pounds_py_column: str = 'Pounds_PY'
    zone_column: str = 'Zone_Suffix_Numeric'
    
    # Filtering columns (NEW!)
    company_number_column: str = 'Company Number'  # â† ADD THIS
    company_region_id_column: str = 'Company Region ID'    # â† ADD THIS
    
    # Optional grouping columns (toggle on/off)
    use_attribute_group: bool = True
    attribute_group_column: str = 'Attribute Group ID'
    
    use_business_center: bool = False
    business_center_column: str = 'Business Center ID'
    
    use_item_group: bool = False
    item_group_column: str = 'Item Group ID'
    
    use_cuisine: bool = True
    cuisine_column: str = 'NPD Cuisine Type'
    
    # Price source (usually CPA only)
    use_price_source: bool = True
    price_source_column: str = 'Price Source Type'
    
    def get_grouping_columns(self) -> List[str]:
        """Get list of columns to use for combo grouping."""
        cols = [self.company_column]
        
        if self.use_cuisine:
            cols.append(self.cuisine_column)
        if self.use_business_center:
            cols.append(self.business_center_column)
        if self.use_attribute_group:
            cols.append(self.attribute_group_column)
        if self.use_item_group:
            cols.append(self.item_group_column)
        if self.use_price_source:
            cols.append(self.price_source_column)
        
        return cols
    
    def validate_dataframe(self, df: pd.DataFrame) -> Tuple[bool, List[str]]:
        """Check if dataframe has required columns."""
        missing = []
        
        # Check required columns
        required = [
            self.company_column,
            self.customer_id_column,
            self.last_invoice_date_column,
            self.pounds_cy_column
        ]
        
        for col in required:
            if col not in df.columns:
                missing.append(col)
        
        # Check enabled optional columns
        if self.use_attribute_group and self.attribute_group_column not in df.columns:
            missing.append(f"{self.attribute_group_column} (enabled but missing)")
        if self.use_business_center and self.business_center_column not in df.columns:
            missing.append(f"{self.business_center_column} (enabled but missing)")
        if self.use_item_group and self.item_group_column not in df.columns:
            missing.append(f"{self.item_group_column} (enabled but missing)")
        
        return (len(missing) == 0, missing)


@dataclass
class InputConfiguration:
    """
    Configure input file paths.
    
    Simple explanation:
    "Tell the system where your data files are. Paste the file paths here."
    """
    
    # Current week data (required)
    current_data_path: str = ""
    
    # Historical data (optional, but recommended)
    historical_data_paths: List[str] = None
    
    # Output settings
    output_directory: str = r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing"
    output_name_prefix: str = "zone_optimization"
    
    # Learning file (where we save state)
    learning_file_path: str = None  # Auto-generated if None
    
    def __post_init__(self):
        if self.historical_data_paths is None:
            self.historical_data_paths = []
        
        # Auto-generate learning file path
        if self.learning_file_path is None:
            self.learning_file_path = os.path.join(
                self.output_directory,
                "zone_optimization_learning_state.json"
            )
    
    def validate_paths(self) -> Tuple[bool, List[str]]:
        """Check if paths exist."""
        issues = []
        
        if not self.current_data_path:
            issues.append("Current data path not specified")
        elif not os.path.exists(self.current_data_path):
            issues.append(f"Current data file not found: {self.current_data_path}")
        
        for path in self.historical_data_paths:
            if not os.path.exists(path):
                issues.append(f"Historical file not found: {path}")
        
        if not os.path.exists(self.output_directory):
            try:
                os.makedirs(self.output_directory)
            except Exception as e:
                issues.append(f"Cannot create output directory: {e}")
        
        return (len(issues) == 0, issues)


class FoodserviceConfig:
    """Main configuration class - combines all settings."""
    
    # Customer activity thresholds (in days)
    LAPSED_FROM_CATEGORY_DAYS = 45  # 6-7 weeks
    LOST_CUSTOMER_DAYS = 60  # 8-9 weeks
    
    # Analysis windows (in weeks)
    REACTIVE_LOOKBACK_WEEKS = 6
    BEHAVIOR_WINDOW_WEEKS = 12
    YOY_LOOKBACK_WEEKS = 8  # â† NEW: Configurable YoY comparison window
    
    # Recommendation thresholds
    MIN_VOLUME_FOR_ACTION = 1000
    HIGH_RECOVERY_THRESHOLD = 0.30
    
    # Filtering (NEW!)
    FILTER_BY_company_number: Optional[str] = None  # Set to specific Company Number or None for all
    FILTER_BY_company_region_id: Optional[str] = None   # Set to specific Company Region ID or None for all
    
    def __init__(self, 
                 data_config: Optional[DataConfiguration] = None,
                 input_config: Optional[InputConfiguration] = None,
                 yoy_lookback_weeks: int = 8,
                 filter_company_number: Optional[str] = None,
                 filter_company_region_id: Optional[str] = None):
        
        self.data_config = data_config or DataConfiguration()
        self.input_config = input_config or InputConfiguration()
        
        # Override defaults with parameters
        self.YOY_LOOKBACK_WEEKS = yoy_lookback_weeks
        self.FILTER_BY_company_number = filter_company_number
        self.FILTER_BY_company_region_id = filter_company_region_id
    
    @classmethod
    def get_timestamp(cls):
        return datetime.now().strftime("%Y%m%d_%H%M%S")

# ============================================================================
# LEARNING SYSTEM
# ============================================================================

@dataclass
class RecommendationRecord:
    """Track a single recommendation over time."""
    
    recommendation_id: str  # Unique ID
    combo_key: str
    company_name: str
    category_description: str
    
    # Recommendation details
    date_recommended: str
    from_zone: int
    to_zone: int
    recommendation_type: str
    
    # Predicted outcomes
    predicted_volume_lift: float
    predicted_customer_recovery: int
    predicted_timeline_weeks: int
    
    # Actual outcomes (filled in later)
    date_implemented: Optional[str] = None
    was_implemented: bool = False
    actual_volume_lift: Optional[float] = None
    actual_customer_recovery: Optional[int] = None
    weeks_to_result: Optional[int] = None
    
    # Learning
    outcome_vs_prediction: Optional[str] = None  # 'BETTER', 'AS_EXPECTED', 'WORSE'
    lessons_learned: List[str] = None
    
    def __post_init__(self):
        if self.lessons_learned is None:
            self.lessons_learned = []


class LearningEngine:
    """
    Tracks recommendations over time and learns from outcomes.
    
    Simple explanation:
    "This is the system's memory. Every time we make a recommendation, we write 
    it down. Every time we check results, we compare what happened vs what we 
    predicted. Over time, the system gets smarter about what works."
    """
    
    def __init__(self, learning_file_path: str):
        self.learning_file_path = learning_file_path
        self.recommendations: Dict[str, RecommendationRecord] = {}
        self.performance_history: List[Dict] = []
        self.zone_effectiveness_learnings: Dict[str, Dict] = defaultdict(dict)
        
        # Load existing learning if available
        self.load_state()
    
    def save_recommendation(self, rec: Dict, predicted_outcomes: Dict) -> str:
        """Save a new recommendation to learning system."""
        
        # Generate unique ID
        rec_id = f"{rec['company_combo']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        record = RecommendationRecord(
            recommendation_id=rec_id,
            combo_key=rec['company_combo'],
            company_name=rec['company_name'],
            category_description=f"{rec.get('cuisine', 'N/A')} - AG{rec.get('attribute_group', 'N/A')}",
            date_recommended=datetime.now().strftime('%Y-%m-%d'),
            from_zone=int(rec['current_zone']),
            to_zone=int(rec['recommended_zone']),
            recommendation_type=rec['recommendation_type'],
            predicted_volume_lift=float(predicted_outcomes.get('volume_lift', 0)),
            predicted_customer_recovery=int(predicted_outcomes.get('customer_recovery', 0)),
            predicted_timeline_weeks=int(predicted_outcomes.get('timeline_weeks', 6))
        )
        
        self.recommendations[rec_id] = record
        self.save_state()
        
        return rec_id
    
    def update_recommendation_outcome(self, 
                                     rec_id: str,
                                     actual_volume_lift: float,
                                     actual_customer_recovery: int,
                                     weeks_elapsed: int):
        """Update a recommendation with actual outcomes."""
        
        if rec_id not in self.recommendations:
            print(f"âš ï¸  Warning: Recommendation {rec_id} not found in learning system")
            return
        
        rec = self.recommendations[rec_id]
        rec.was_implemented = True
        rec.date_implemented = datetime.now().strftime('%Y-%m-%d')
        rec.actual_volume_lift = actual_volume_lift
        rec.actual_customer_recovery = actual_customer_recovery
        rec.weeks_to_result = weeks_elapsed
        
        # Assess outcome vs prediction
        volume_ratio = actual_volume_lift / rec.predicted_volume_lift if rec.predicted_volume_lift > 0 else 0
        
        if volume_ratio >= 1.2:
            rec.outcome_vs_prediction = 'BETTER_THAN_EXPECTED'
            rec.lessons_learned.append(f"ðŸŽ‰ Exceeded prediction by {(volume_ratio - 1) * 100:.0f}%")
        elif volume_ratio >= 0.8:
            rec.outcome_vs_prediction = 'AS_EXPECTED'
            rec.lessons_learned.append("âœ… Performed as predicted")
        else:
            rec.outcome_vs_prediction = 'WORSE_THAN_EXPECTED'
            rec.lessons_learned.append(f"âš ï¸  Underperformed prediction by {(1 - volume_ratio) * 100:.0f}%")
        
        # Learn zone effectiveness
        self._update_zone_effectiveness(rec)
        
        self.save_state()
    
    def _update_zone_effectiveness(self, rec: RecommendationRecord):
        """Learn which zone changes work best."""
        
        # Key by recommendation type + zone change
        pattern_key = f"{rec.recommendation_type}_{rec.from_zone}_to_{rec.to_zone}"
        
        if pattern_key not in self.zone_effectiveness_learnings:
            self.zone_effectiveness_learnings[pattern_key] = {
                'attempts': 0,
                'successes': 0,
                'total_volume_lift': 0,
                'total_customer_recovery': 0,
                'avg_weeks_to_result': 0
            }
        
        learning = self.zone_effectiveness_learnings[pattern_key]
        learning['attempts'] += 1
        
        if rec.outcome_vs_prediction in ['BETTER_THAN_EXPECTED', 'AS_EXPECTED']:
            learning['successes'] += 1
        
        learning['total_volume_lift'] += rec.actual_volume_lift or 0
        learning['total_customer_recovery'] += rec.actual_customer_recovery or 0
        
        # Running average of weeks to result
        n = learning['attempts']
        learning['avg_weeks_to_result'] = (
            (learning['avg_weeks_to_result'] * (n - 1) + rec.weeks_to_result) / n
        )
    
    def get_pattern_confidence(self, recommendation_type: str, from_zone: int, to_zone: int) -> float:
        """Get confidence score for a recommendation pattern based on past results."""
        
        pattern_key = f"{recommendation_type}_{from_zone}_to_{to_zone}"
        
        if pattern_key not in self.zone_effectiveness_learnings:
            return 0.5  # Neutral confidence if no history
        
        learning = self.zone_effectiveness_learnings[pattern_key]
        
        if learning['attempts'] == 0:
            return 0.5
        
        # Confidence based on success rate and sample size
        success_rate = learning['successes'] / learning['attempts']
        sample_size_factor = min(learning['attempts'] / 10, 1.0)  # Max out at 10 attempts
        
        confidence = (success_rate * 0.7) + (sample_size_factor * 0.3)
        
        return confidence
    
    def get_expected_timeline(self, recommendation_type: str, from_zone: int, to_zone: int) -> int:
        """Get expected timeline based on past results."""
        
        pattern_key = f"{recommendation_type}_{from_zone}_to_{to_zone}"
        
        if pattern_key in self.zone_effectiveness_learnings:
            avg_weeks = self.zone_effectiveness_learnings[pattern_key]['avg_weeks_to_result']
            if avg_weeks > 0:
                return int(avg_weeks)
        
        # Default based on recommendation type
        defaults = {
            'HIGH_RECOVERY_POTENTIAL': 3,
            'REACTIVE_CORRECTION': 6,
            'PEER_CONSENSUS': 5,
            'NEEDS_FRACTIONAL_ZONES': 10
        }
        
        return defaults.get(recommendation_type, 6)
    
    def get_pending_recommendations(self) -> List[RecommendationRecord]:
        """Get recommendations that haven't been updated with outcomes."""
        return [rec for rec in self.recommendations.values() if not rec.was_implemented]
    
    def get_completed_recommendations(self) -> List[RecommendationRecord]:
        """Get recommendations with outcome data."""
        return [rec for rec in self.recommendations.values() if rec.was_implemented]
    
    def generate_learning_report(self) -> Dict:
        """Generate summary of what we've learned."""
        
        completed = self.get_completed_recommendations()
        
        if not completed:
            return {
                'total_recommendations_tracked': len(self.recommendations),
                'completed_recommendations': 0,
                'message': 'No completed recommendations yet - check back after implementation'
            }
        
        better_than_expected = sum(1 for r in completed if r.outcome_vs_prediction == 'BETTER_THAN_EXPECTED')
        as_expected = sum(1 for r in completed if r.outcome_vs_prediction == 'AS_EXPECTED')
        worse_than_expected = sum(1 for r in completed if r.outcome_vs_prediction == 'WORSE_THAN_EXPECTED')
        
        total_volume_lift = sum(r.actual_volume_lift for r in completed if r.actual_volume_lift)
        total_customers_recovered = sum(r.actual_customer_recovery for r in completed if r.actual_customer_recovery)
        
# ============================================================================
# CONTINUATION FROM WHERE IT CUT OFF
# ============================================================================

        # Best performing patterns
        pattern_success_rates = {}
        for pattern_key, learning in self.zone_effectiveness_learnings.items():
            if learning['attempts'] > 0:
                pattern_success_rates[pattern_key] = {
                    'pattern': pattern_key,
                    'success_rate': f"{(learning['successes'] / learning['attempts']) * 100:.0f}%",
                    'attempts': learning['attempts'],
                    'avg_volume_lift': learning['total_volume_lift'] / learning['attempts'],
                    'avg_weeks': learning['avg_weeks_to_result']
                }
        
        return {
            'total_recommendations_tracked': len(self.recommendations),
            'completed_recommendations': len(completed),
            'better_than_expected': better_than_expected,
            'as_expected': as_expected,
            'worse_than_expected': worse_than_expected,
            'total_volume_lift_achieved': total_volume_lift,
            'total_customers_recovered': total_customers_recovered,
            'success_rate': f"{((better_than_expected + as_expected) / len(completed)) * 100:.0f}%",
            'best_performing_patterns': sorted(
                pattern_success_rates.values(), 
                key=lambda x: x['avg_volume_lift'], 
                reverse=True
            )[:5]
        }
    
    def save_state(self):
        """Save learning state to JSON file."""
        try:
            state = {
                'last_updated': datetime.now().isoformat(),
                'recommendations': {
                    rec_id: asdict(rec) 
                    for rec_id, rec in self.recommendations.items()
                },
                'zone_effectiveness_learnings': dict(self.zone_effectiveness_learnings)
            }
            
            with open(self.learning_file_path, 'w') as f:
                json.dump(state, f, indent=2)
                
        except Exception as e:
            print(f"âš ï¸  Warning: Could not save learning state: {e}")
    
    def load_state(self):
        """Load learning state from JSON file."""
        if not os.path.exists(self.learning_file_path):
            print(f"   â„¹ï¸  No previous learning state found (this is normal for first run)")
            return
        
        try:
            with open(self.learning_file_path, 'r') as f:
                state = json.load(f)
            
            # Reconstruct recommendation records
            for rec_id, rec_dict in state.get('recommendations', {}).items():
                self.recommendations[rec_id] = RecommendationRecord(**rec_dict)
            
            # Reconstruct zone effectiveness learnings
            self.zone_effectiveness_learnings = defaultdict(
                dict, 
                state.get('zone_effectiveness_learnings', {})
            )
            
            print(f"   âœ… Loaded learning state from {state.get('last_updated', 'unknown date')}")
            print(f"   ðŸ“š {len(self.recommendations)} recommendations in memory")
            
        except Exception as e:
            print(f"âš ï¸  Warning: Could not load learning state: {e}")

# ============================================================================
# CUSTOMER ACTIVITY CLASSIFICATION
# ============================================================================

def classify_customer_activity(df: pd.DataFrame, 
                               config: FoodserviceConfig = None) -> pd.DataFrame:
    """
    Classify customers into categories for recovery targeting.
    
    8th Grade Explanation:
    "We group customers into 3 buckets:
    1. ACTIVE: Still buying this category (yay!)
    2. LAPSED: Still our customer, but stopped buying THIS category (OPPORTUNITY!)
    3. LOST: Haven't bought anything in 2+ months (moved on)"
    """
    if config is None:
        config = FoodserviceConfig()
    
    df = df.copy()
    
    # Validate required columns
    if config.data_config.last_invoice_date_column not in df.columns:
        raise ValueError(f"âŒ Missing '{config.data_config.last_invoice_date_column}' - CRITICAL!")
    
    # Parse dates
    df['Last_Invoice_Date'] = pd.to_datetime(
        df[config.data_config.last_invoice_date_column], 
        errors='coerce'
    )
    max_date = df['Last_Invoice_Date'].max()
    
    if pd.isna(max_date):
        raise ValueError("âŒ No valid dates in Last Invoice Date column")
    
    df['Days_Since_Last_Purchase'] = (max_date - df['Last_Invoice_Date']).dt.days
    
    # Ensure numeric volume
    df['Pounds_CY'] = pd.to_numeric(
        df.get(config.data_config.pounds_cy_column, 0), 
        errors='coerce'
    ).fillna(0)
    
    # Classification logic
    def classify(row):
        days_since = row['Days_Since_Last_Purchase']
        has_current_volume = row['Pounds_CY'] > 0
        
        if has_current_volume:
            return 'ACTIVE_BUYER'
        elif days_since <= config.LAPSED_FROM_CATEGORY_DAYS:
            return 'LAPSED_FROM_CATEGORY'  # â† THE GOLD MINE!
        elif days_since <= config.LOST_CUSTOMER_DAYS:
            return 'RECENTLY_LOST'
        else:
            return 'LOST_CUSTOMER'
    
    df['Customer_Status'] = df.apply(classify, axis=1)
    
    # Add recovery potential score
    df['Recovery_Potential'] = df['Customer_Status'].map({
        'ACTIVE_BUYER': 0,
        'LAPSED_FROM_CATEGORY': 10,
        'RECENTLY_LOST': 5,
        'LOST_CUSTOMER': 0
    })
    
    return df

def calculate_purchase_consistency(df: pd.DataFrame, 
                                   config: FoodserviceConfig) -> pd.DataFrame:
    """
    Measure week-over-week purchase consistency - THE GREEN FLAG!
    
    8th Grade Explanation:
    "If a customer buys from us EVERY WEEK for 8 weeks straight at Zone 3, 
    that's a GREEN FLAG that Zone 3 works! If they buy once then disappear, 
    that's a RED FLAG."
    
    This catches the 'stickiness' of a zone.
    """
    
    df = df.copy()
    
    # Ensure we have what we need
    if 'Pounds_CY' not in df.columns:
        df['Pounds_CY'] = 0
    
    df['Pounds_CY'] = pd.to_numeric(df['Pounds_CY'], errors='coerce').fillna(0)
    
    # Group by combo + zone + customer + week
    customer_weeks = df.groupby([
        'Company_Combo_Key',
        config.data_config.zone_column,
        config.data_config.customer_id_column,
        config.data_config.fiscal_week_column
    ], dropna=False).agg({
        'Pounds_CY': 'sum'
    }).reset_index()
    
    # Mark active weeks (any purchase > 0)
    customer_weeks['active_week'] = (customer_weeks['Pounds_CY'] > 0).astype(int)
    
    # Calculate consistency metrics per customer per combo/zone
    consistency = customer_weeks.groupby([
        'Company_Combo_Key',
        config.data_config.zone_column,
        config.data_config.customer_id_column
    ], dropna=False).agg({
        config.data_config.fiscal_week_column: 'nunique',  # Total weeks present
        'active_week': 'sum'  # Weeks with purchases
    }).reset_index()
    
    consistency.columns = [
        'Company_Combo_Key', 'Zone', 'Customer_ID', 
        'weeks_present', 'weeks_active'
    ]
    
    # Calculate consistency rate (% of weeks they bought when present)
    consistency['consistency_rate'] = (
        consistency['weeks_active'] / consistency['weeks_present']
    ).fillna(0)
    
    # GREEN FLAG: Consistent buyers (bought in 75%+ of weeks present)
    consistency['consistent_buyer'] = (consistency['consistency_rate'] >= 0.75).astype(int)
    
    # Aggregate to zone level
    zone_consistency = consistency.groupby([
        'Company_Combo_Key', 'Zone'
    ], dropna=False).agg({
        'Customer_ID': 'nunique',  # DISTINCT customers
        'consistent_buyer': 'sum',  # How many are consistent
        'consistency_rate': 'mean',  # Average consistency
        'weeks_active': 'mean'  # Average active weeks per customer
    }).reset_index()
    
    zone_consistency.columns = [
        'Company_Combo_Key', 'Zone',
        'distinct_customers',
        'consistent_buyers',
        'avg_consistency_rate',
        'avg_active_weeks'
    ]
    
    # Calculate GREEN FLAG rate
    zone_consistency['green_flag_rate'] = (
        zone_consistency['consistent_buyers'] / 
        zone_consistency['distinct_customers'].replace(0, 1)
    )
    
    return zone_consistency

def calculate_yoy_customer_metrics(df: pd.DataFrame, 
                                   config: FoodserviceConfig) -> Dict:
    """
    Calculate year-over-year distinct customer metrics using configurable lookback window.
    
    Simple explanation:
    "Compare the last X weeks this year vs. the same X weeks last year. 
    How many distinct customers are we keeping?"
    
    Example: If lookback = 8 weeks and we're at Week 52:
    - This Year: Weeks 45-52 (last 8 weeks)
    - Last Year: Weeks 45-52 (same period last year)
    """
    
    df = df.copy()
    
    # Ensure we have fiscal week data
    if config.data_config.fiscal_week_column not in df.columns:
        print("   âš ï¸  Warning: No fiscal week column, can't calculate YoY with lookback")
        return {}
    
    # Ensure numeric
    df['Fiscal_Week'] = pd.to_numeric(df[config.data_config.fiscal_week_column], errors='coerce')
    df = df.dropna(subset=['Fiscal_Week'])
    df['Fiscal_Week'] = df['Fiscal_Week'].astype(int)
    
    # Ensure we have Pounds columns
    if 'Pounds_CY' not in df.columns or config.data_config.pounds_py_column not in df.columns:
        print("   âš ï¸  Warning: Missing Pounds_CY or Pounds_PY column")
        return {}
    
    df['Pounds_CY'] = pd.to_numeric(df['Pounds_CY'], errors='coerce').fillna(0)
    df['Pounds_PY'] = pd.to_numeric(df[config.data_config.pounds_py_column], errors='coerce').fillna(0)
    
    # Get latest week and calculate lookback window
    latest_week = df['Fiscal_Week'].max()
    lookback_weeks = config.YOY_LOOKBACK_WEEKS
    cutoff_week = latest_week - lookback_weeks + 1
    
    print(f"   ðŸ“… Comparing Weeks {cutoff_week}-{latest_week} (last {lookback_weeks} weeks)")
    
    # Filter to lookback window
    recent_data = df[df['Fiscal_Week'] >= cutoff_week].copy()
    
    if recent_data.empty:
        print("   âš ï¸  No data in lookback window")
        return {}
    
    # Identify distinct customers by year (in the lookback window)
    cy_customers = recent_data[recent_data['Pounds_CY'] > 0][config.data_config.customer_id_column].unique()
    py_customers = recent_data[recent_data['Pounds_PY'] > 0][config.data_config.customer_id_column].unique()
    
    # Calculate overlaps
    retained_customers = set(cy_customers) & set(py_customers)
    new_customers = set(cy_customers) - set(py_customers)
    lost_customers = set(py_customers) - set(cy_customers)
    
    # Overall metrics
    overall = {
        'lookback_weeks': lookback_weeks,
        'week_range': f"{cutoff_week}-{latest_week}",
        'distinct_customers_cy': len(cy_customers),
        'distinct_customers_py': len(py_customers),
        'customer_change': len(cy_customers) - len(py_customers),
        'customer_change_pct': ((len(cy_customers) - len(py_customers)) / len(py_customers) * 100) if len(py_customers) > 0 else 0,
        'retained_customers': len(retained_customers),
        'retention_rate': (len(retained_customers) / len(py_customers) * 100) if len(py_customers) > 0 else 0,
        'new_customers': len(new_customers),
        'lost_customers': len(lost_customers),
        'customer_status_summary': {
            'RETAINED': len(retained_customers),
            'NEW': len(new_customers),
            'LOST': len(lost_customers)
        }
    }
    
    # By combo breakdown
    combo_metrics = []
    
    for combo in recent_data['Company_Combo_Key'].unique():
        combo_data = recent_data[recent_data['Company_Combo_Key'] == combo]
        
        combo_cy = combo_data[combo_data['Pounds_CY'] > 0][config.data_config.customer_id_column].unique()
        combo_py = combo_data[combo_data['Pounds_PY'] > 0][config.data_config.customer_id_column].unique()
        
        combo_retained = set(combo_cy) & set(combo_py)
        combo_new = set(combo_cy) - set(combo_py)
        combo_lost = set(combo_py) - set(combo_cy)
        
        combo_metrics.append({
            'Company_Combo_Key': combo,
            'distinct_customers_cy': len(combo_cy),
            'distinct_customers_py': len(combo_py),
            'customer_change': len(combo_cy) - len(combo_py),
            'customer_change_pct': ((len(combo_cy) - len(combo_py)) / len(combo_py) * 100) if len(combo_py) > 0 else 0,
            'retained_customers': len(combo_retained),
            'retention_rate': (len(combo_retained) / len(combo_py) * 100) if len(combo_py) > 0 else 0,
            'new_customers': len(combo_new),
            'lost_customers': len(combo_lost)
        })
    
    overall['by_combo'] = pd.DataFrame(combo_metrics)
    
    return overall

def apply_filters(df: pd.DataFrame, config: FoodserviceConfig) -> pd.DataFrame:
    """
    Apply Company Number and/or Company Region ID filters.
    
    Simple explanation:
    "If I only want to see recommendations for Central Texas (Company Number = '10'), 
    filter everything else out. Or if I want only Northeast Region, show just that."
    """
    
    df = df.copy()
    original_count = len(df)
    
    # Filter by Company Number
    if config.FILTER_BY_company_number:
        if config.data_config.company_number_column in df.columns:
            df = df[df[config.data_config.company_number_column].astype(str) == str(config.FILTER_BY_company_number)]
            print(f"   ðŸ” Filtered to Company Number '{config.FILTER_BY_company_number}': {len(df):,} rows (was {original_count:,})")
        else:
            print(f"   âš ï¸  Warning: Company Number column '{config.data_config.company_number_column}' not found")
    
    # Filter by Company Region ID
    if config.FILTER_BY_company_region_id:
        if config.data_config.company_region_id_column in df.columns:
            df = df[df[config.data_config.company_region_id_column].astype(str) == str(config.FILTER_BY_company_region_id)]
            print(f"   ðŸ” Filtered to Company Region ID '{config.FILTER_BY_company_region_id}': {len(df):,} rows (was {original_count:,})")
        else:
            print(f"   âš ï¸  Warning: Company Region ID column '{config.data_config.company_region_id_column}' not found")
    
    if df.empty:
        print(f"   âŒ WARNING: Filters removed all data! Check your Company/Company Region IDs.")
    
    return df
# ============================================================================
# REACTIVE PRICING DETECTION
# ============================================================================

def detect_reactive_pricing_failures(df: pd.DataFrame, 
                                    config: FoodserviceConfig = None) -> Dict:
    """
    Find combos where we overpriced, lost customers, then dropped zone reactively.
    
    8th Grade Explanation:
    "Sometimes we charge too much, restaurants stop buying, then we panic and drop 
    the price. The lower zone looks good, but only because we already killed the 
    business at the higher zone."
    """
    if config is None:
        config = FoodserviceConfig()
    
    # Need customer status first
    if 'Customer_Status' not in df.columns:
        df = classify_customer_activity(df, config)
    
    flags = {}
    
    for combo, g in df.groupby('Company_Combo_Key', dropna=False):
        g = g.sort_values(config.data_config.fiscal_week_column)
        
        # Must have zone info
        if config.data_config.zone_column not in g.columns:
            continue
            
        g[config.data_config.zone_column] = pd.to_numeric(
            g[config.data_config.zone_column], 
            errors='coerce'
        )
        g = g.dropna(subset=[config.data_config.zone_column])
        
        if len(g) < 2:
            continue
        
        # Find first zone drop
        zone_diff = g[config.data_config.zone_column].diff()
        zone_drops = g[zone_diff < 0]
        
        if zone_drops.empty:
            continue
        
        # Analyze first drop
        first_drop = zone_drops.iloc[0]
        drop_week = first_drop[config.data_config.fiscal_week_column]
        drop_idx = g[g[config.data_config.fiscal_week_column] == drop_week].index[0]
        
        # Get zones before and after
        pre_drop_data = g.loc[g.index < drop_idx]
        if len(pre_drop_data) == 0:
            continue
            
        pre_drop_zone = pre_drop_data[config.data_config.zone_column].iloc[-1]
        post_drop_zone = first_drop[config.data_config.zone_column]
        
        # Get pre-drop window
        lookback = config.REACTIVE_LOOKBACK_WEEKS
        pre_window = g[
            g[config.data_config.fiscal_week_column].between(
                drop_week - lookback, 
                drop_week - 1
            )
        ]
        
        if pre_window.empty:
            continue
        
        # KEY METRIC: How many customers lapsed BEFORE the zone drop?
        status_counts = pre_window['Customer_Status'].value_counts()
        total_customers = pre_window[config.data_config.customer_id_column].nunique()
        
        lapsed_count = status_counts.get('LAPSED_FROM_CATEGORY', 0)
        lapsed_pct = (lapsed_count / total_customers) if total_customers > 0 else 0
        
        # If 30%+ customers lapsed before drop, it's reactive!
        if lapsed_pct >= 0.30:
            
            # Measure recovery after drop
            post_window = g[
                g[config.data_config.fiscal_week_column].between(
                    drop_week, 
                    drop_week + lookback
                )
            ]
            
            pre_volume = pre_window['Pounds_CY'].sum()
            post_volume = post_window['Pounds_CY'].sum()
            recovery_pct = ((post_volume - pre_volume) / pre_volume) if pre_volume > 0 else 0
            
            # Calculate likely true optimal
            likely_optimal = int(pre_drop_zone) - 1
            if likely_optimal < 1:
                likely_optimal = 1
            
            flags[combo] = {
                'reactive_downzone': True,
                'from_zone': int(pre_drop_zone),
                'to_zone': int(post_drop_zone),
                'customers_lapsed_before_drop': int(lapsed_count),
                'lapsed_percentage': lapsed_pct,
                'volume_recovery_rate': recovery_pct,
                'likely_true_optimal': likely_optimal,
                'stakeholder_message': (
                    f"âš ï¸ REACTIVE: {int(lapsed_count)} customers stopped buying "
                    f"at Zone {int(pre_drop_zone)}. We dropped to Zone {int(post_drop_zone)} "
                    f"to recover them. Real optimal is likely Zone {likely_optimal}."
                ),
                'trust_level': 'MEDIUM'
            }
    
    return flags

# ============================================================================
# ZONE OPTIMIZATION ENGINE
# ============================================================================

class FoodserviceZoneEngine:
    """Enhanced with purchase consistency tracking."""
    
    def __init__(self, 
                 config: FoodserviceConfig,
                 learning_engine: Optional[LearningEngine] = None):
        self.config = config
        self.learning_engine = learning_engine
        self.reactive_flags = {}
        self.customer_analysis = None
        self.consistency_analysis = None 
        self.yoy_customer_metrics = None
    
    def analyze_current_state(self, df: pd.DataFrame) -> pd.DataFrame:
            """Prepare current state data with all needed features."""
            df = df.copy()
            
            # Classify customers
            df = classify_customer_activity(df, self.config)
            
            # Calculate purchase consistency (THE GREEN FLAG!)
            self.consistency_analysis = calculate_purchase_consistency(df, self.config)
            print(f"   ðŸŸ¢ Calculated consistency for {len(self.consistency_analysis)} zone combinations")
            
            # Detect reactive pricing patterns
            self.reactive_flags = detect_reactive_pricing_failures(df, self.config)
            
            # Store for later use
            self.customer_analysis = self._build_customer_summary(df)
            
            return df
    
    def _build_customer_summary(self, df: pd.DataFrame) -> pd.DataFrame:
        """Aggregate customer metrics by combo and zone."""
        
        grouping_cols = self.config.data_config.get_grouping_columns()
        grouping_cols.append(self.config.data_config.zone_column)
        
        summary = df.groupby(grouping_cols, dropna=False).agg({
            self.config.data_config.customer_id_column: 'nunique',
            'Customer_Status': lambda x: (x == 'ACTIVE_BUYER').sum(),
            'Recovery_Potential': 'sum',
            'Pounds_CY': 'sum',
            self.config.data_config.pounds_py_column: 'sum'
        }).reset_index()
        
        summary.columns = [*grouping_cols, 
                          'Total_Customers', 'Active_Customers', 'Recovery_Potential_Score',
                          'Volume_CY', 'Volume_PY']
        
        # Calculate key metrics
        summary['Lapsed_Customers'] = summary['Total_Customers'] - summary['Active_Customers']
        summary['Lapsed_Pct'] = summary['Lapsed_Customers'] / summary['Total_Customers']
        summary['Volume_Change'] = summary['Volume_CY'] - summary['Volume_PY']
        summary['Volume_Change_Pct'] = summary['Volume_Change'] / summary['Volume_PY'].replace(0, 1)
        
        return summary
    
    def generate_recommendations(self, 
                                current_df: pd.DataFrame, 
                                historical_df: Optional[pd.DataFrame] = None) -> List[Dict]:
        """Generate zone recommendations with recovery potential prioritization."""
        
        # Analyze current state
        current_df = self.analyze_current_state(current_df)
        
        recommendations = []
        
        # Get current zone by combo
        grouping_cols = self.config.data_config.get_grouping_columns()
        
        current_zones = (current_df.groupby('Company_Combo_Key')
                        .agg({
                            self.config.data_config.zone_column: lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],
                            **{col: 'first' for col in grouping_cols}
                        })
                        .reset_index())
        
        for _, row in current_zones.iterrows():
            combo = row['Company_Combo_Key']
            current_zone = int(row[self.config.data_config.zone_column])
            
# Get customer breakdown - DISTINCT customers only
            combo_data = current_df[current_df['Company_Combo_Key'] == combo]
            
            status_counts = combo_data['Customer_Status'].value_counts()
            
            # CRITICAL: Count DISTINCT customers (handles multiple rows per customer)
            total_customers = combo_data[self.config.data_config.customer_id_column].nunique()
            
            # Get status counts per DISTINCT customer
            customer_status = (combo_data
                             .groupby(self.config.data_config.customer_id_column)['Customer_Status']
                             .first())  # Each customer's status
            
            active_customers = (customer_status == 'ACTIVE_BUYER').sum()
            lapsed_customers = (customer_status == 'LAPSED_FROM_CATEGORY').sum()
            
            # Get consistency metrics (THE GREEN FLAG!)
            consistency_data = self.consistency_analysis[
                (self.consistency_analysis['Company_Combo_Key'] == combo) &
                (self.consistency_analysis['Zone'] == current_zone)
            ]
            
            if not consistency_data.empty:
                green_flag_rate = consistency_data['green_flag_rate'].iloc[0]
                consistent_buyers = int(consistency_data['consistent_buyers'].iloc[0])
                avg_consistency = consistency_data['avg_consistency_rate'].iloc[0]
            else:
                green_flag_rate = 0
                consistent_buyers = 0
                avg_consistency = 0
            active_customers = status_counts.get('ACTIVE_BUYER', 0)
            lapsed_customers = status_counts.get('LAPSED_FROM_CATEGORY', 0)
            
            total_volume = combo_data['Pounds_CY'].sum()
            
            # Skip if too small
            if total_volume < self.config.MIN_VOLUME_FOR_ACTION:
                continue
            
            lapsed_pct = (lapsed_customers / total_customers) if total_customers > 0 else 0
            
            # Build recommendation
# Build recommendation
            rec = {
                'company_combo': combo,
                'company_name': row[self.config.data_config.company_column],
                'cuisine': row.get(self.config.data_config.cuisine_column, 'N/A'),
                'attribute_group': row.get(self.config.data_config.attribute_group_column, 'N/A'),
                'current_zone': current_zone,
                'recommended_zone': current_zone,
                'total_customers': int(total_customers),
                'active_customers': int(active_customers),
                'lapsed_customers': int(lapsed_customers),
                'lapsed_pct': lapsed_pct,
                'total_volume': total_volume,
                'implementation_priority': 0,
                'consistent_buyers': consistent_buyers,
                'green_flag_rate': green_flag_rate,
                'avg_consistency': avg_consistency
            }
            
            # Determine recommendation type and zone
            if lapsed_pct >= self.config.HIGH_RECOVERY_THRESHOLD:
                # HIGH RECOVERY POTENTIAL
                rec.update({
                    'recommendation_type': 'HIGH_RECOVERY_POTENTIAL',
                    'recommended_zone': max(1, current_zone - 1),
                    'stakeholder_message': (
                        f"ðŸŽ¯ EASY WIN: {int(lapsed_customers)} restaurants still buy from us "
                        f"but stopped buying this category. Better pricing will bring them back!"
                    ),
                    'expected_result': f"Win back {int(lapsed_customers * 0.6)} customers = +{int(total_volume * 0.3):,} lbs",
                    'timeline': 'âš¡ 2-4 weeks',
                    'risk_level': 'âœ… LOW',
                    'implementation_priority': 90
                })
            
            elif combo in self.reactive_flags:
                # REACTIVE CORRECTION
                flag = self.reactive_flags[combo]
                rec.update({
                    'recommendation_type': 'REACTIVE_CORRECTION',
                    'recommended_zone': flag['likely_true_optimal'],
                    'stakeholder_message': flag['stakeholder_message'],
                    'expected_result': f"Find true optimal = +{int(total_volume * 0.15):,} lbs",
                    'timeline': 'ðŸ“… 4-6 weeks',
                    'risk_level': 'âš ï¸ MEDIUM',
                    'implementation_priority': 75,
                    'reactive_details': flag
                })
            
            elif historical_df is not None:
                # PEER CONSENSUS
                consensus_zone = self._get_consensus_zone(combo, current_zone, historical_df)
                
                if consensus_zone != current_zone and consensus_zone > 0:
                    rec.update({
                        'recommendation_type': 'PEER_CONSENSUS',
                        'recommended_zone': consensus_zone,
                        'stakeholder_message': (
                            f"Other similar operations do best at Zone {consensus_zone}"
                        ),
                        'expected_result': f"Match peers = +{int(total_volume * 0.15):,} lbs",
                        'timeline': 'ðŸ“… 4-6 weeks',
                        'risk_level': 'âœ… LOW',
                        'implementation_priority': 60
                    })
                else:
                    # Consensus matches current zone - HOLD
                    rec.update({
                        'recommendation_type': 'HOLD',
                        'recommended_zone': current_zone,
                        'stakeholder_message': 'Current zone matches peer consensus',
                        'expected_result': 'Maintain current performance',
                        'timeline': 'Monitor monthly',
                        'risk_level': 'âœ… LOW',
                        'implementation_priority': 20
                    })
            
            elif current_zone == 1 and lapsed_pct > 0.15:
                # NEEDS FRACTIONAL ZONES
                rec.update({
                    'recommendation_type': 'NEEDS_FRACTIONAL_ZONES',
                    'recommended_zone': 1,
                    'stakeholder_message': (
                        f"âš ï¸ Already at Zone 1 but still losing {int(lapsed_customers)} customers. "
                        f"Need fractional zones (0.5, 0.7, 0.8)."
                    ),
                    'expected_result': 'Cannot estimate without fractional zones',
                    'timeline': 'ðŸ“… 6-12 weeks',
                    'risk_level': 'âš ï¸ MEDIUM',
                    'implementation_priority': 70,
                    'requires_fractional': True
                })
            
            else:
                # HOLD
                rec.update({
                    'recommendation_type': 'HOLD',
                    'recommended_zone': current_zone,
                    'stakeholder_message': 'Current zone performing well',
                    'expected_result': 'Maintain current performance',
                    'timeline': 'Monitor monthly',
                    'risk_level': 'âœ… LOW',
                    'implementation_priority': 20
                })
            
            # Only add if actionable
            if rec['current_zone'] != rec['recommended_zone'] or rec.get('requires_fractional'):
                recommendations.append(rec)
        
        # Sort by priority
        recommendations.sort(key=lambda x: x['implementation_priority'], reverse=True)
        
        return recommendations
    
    def _get_consensus_zone(self, combo: str, current_zone: int, 
                            historical_df: pd.DataFrame) -> int:
            """
            Find consensus zone from historical data.
            Now HEAVILY weighted toward zones with HIGH CONSISTENCY (GREEN FLAG!)
            """
            
            # Extract parts
            parts = combo.split('_')
            if len(parts) < 3:
                return current_zone
            
            hist = historical_df.copy()
            hist = classify_customer_activity(hist, self.config)
            
            # Calculate consistency for historical data
            hist_consistency = calculate_purchase_consistency(hist, self.config)
            
            # Get all zones for this combo pattern
            zone_perf = hist.groupby(self.config.data_config.zone_column).agg({
                'Pounds_CY': 'sum',
                'Customer_Status': lambda x: (x == 'ACTIVE_BUYER').mean()
            }).reset_index()
            
            zone_perf.columns = ['Zone', 'Total_Volume', 'Active_Rate']
            
            # Merge with consistency data
            zone_perf = zone_perf.merge(
                hist_consistency[['Zone', 'green_flag_rate', 'avg_consistency_rate']],
                on='Zone',
                how='left'
            ).fillna(0)
            
            # NEW SCORING: Heavy weight on consistency (THE GREEN FLAG!)
            zone_perf['Score'] = (
                zone_perf['Total_Volume'] * 0.3 +           # Volume still matters (30%)
                zone_perf['Active_Rate'] * 1000 * 0.3 +     # Active rate (30%)
                zone_perf['green_flag_rate'] * 2000 * 0.4   # ðŸŸ¢ GREEN FLAG! (40%)
            )
            
            if zone_perf.empty:
                return current_zone
            
            best_zone = int(zone_perf.loc[zone_perf['Score'].idxmax(), 'Zone'])
            
            # Never recommend going UP
            if best_zone > current_zone:
                return current_zone
            
            return best_zone

# ============================================================================
# DASHBOARD GENERATORS
# ============================================================================
def create_executive_summary(recommendations: List[Dict], 
                            yoy_metrics: Optional[Dict] = None,
                            config: Optional[FoodserviceConfig] = None) -> Dict:
    """One-page summary with company name."""
    
    high_priority = [r for r in recommendations if r['implementation_priority'] >= 70]
    recovery_opps = [r for r in recommendations if r['recommendation_type'] == 'HIGH_RECOVERY_POTENTIAL']
    fractional_needed = [r for r in recommendations if r.get('requires_fractional', False)]
    
    total_volume = sum(r['total_volume'] for r in high_priority)
    total_lapsed = sum(r['lapsed_customers'] for r in recovery_opps)
    
    summary = {
        'total_opportunities': len(recommendations),
        'high_priority_moves': len(high_priority),
        'easy_wins_lapsed_recovery': len(recovery_opps),
        'fractional_zones_needed': len(fractional_needed),
        'total_volume_at_stake': f"{total_volume:,.0f} lbs",
        'customers_to_win_back': int(total_lapsed),
        'expected_timeframe': '2-8 weeks',
        'first_action': high_priority[0]['stakeholder_message'] if high_priority else 'No immediate actions'
    }
    
    # Add filter info (ENHANCED WITH NAME!)
    if config:
        filter_info = []
        
        # Get company name from recommendations if filtered
        if config.FILTER_BY_company_number and recommendations:
            company_name = recommendations[0].get('company_name', config.FILTER_BY_company_number)
            filter_info.append(f"Company: {company_name} (ID: {config.FILTER_BY_company_number})")
        elif config.FILTER_BY_company_number:
            filter_info.append(f"Company Number: {config.FILTER_BY_company_number}")
        
        if config.FILTER_BY_company_region_id:
            filter_info.append(f"Region: {config.FILTER_BY_company_region_id}")
        
        summary['scope'] = ', '.join(filter_info) if filter_info else 'All Companies & Regions'
        summary['yoy_lookback_window'] = f"{config.YOY_LOOKBACK_WEEKS} weeks"
    
    # Add YoY metrics
    if yoy_metrics:
        summary.update({
            'yoy_comparison_period': yoy_metrics.get('week_range', 'N/A'),
            'distinct_customers_this_year': f"{yoy_metrics['distinct_customers_cy']:,}",
            'distinct_customers_last_year': f"{yoy_metrics['distinct_customers_py']:,}",
            'customer_change_count': f"{yoy_metrics['customer_change']:+,}",
            'customer_change_percent': f"{yoy_metrics['customer_change_pct']:+.1f}%",
            'customer_retention_rate': f"{yoy_metrics['retention_rate']:.1f}%",
            'new_customers_gained': f"{yoy_metrics['new_customers']:,}",
            'customers_lost': f"{yoy_metrics['lost_customers']:,}"
        })
    
    return summary


def create_quick_wins_dashboard(recommendations: List[Dict], 
                               yoy_metrics: Optional[Dict] = None) -> pd.DataFrame:
    """Top 5 moves in plain English with YoY context."""
    
    quick_wins = []
    
    # Get combo-level YoY data
    yoy_by_combo = {}
    if yoy_metrics and 'by_combo' in yoy_metrics:
        for _, row in yoy_metrics['by_combo'].iterrows():
            yoy_by_combo[row['Company_Combo_Key']] = row
    
    for i, rec in enumerate(recommendations[:5], 1):
        
        # Build problem description
        if rec['recommendation_type'] == 'HIGH_RECOVERY_POTENTIAL':
            problem = f"ðŸŽ¯ {rec['lapsed_customers']} restaurants stopped buying but still order other items"
        elif rec['recommendation_type'] == 'REACTIVE_CORRECTION':
            problem = f"âš ï¸ Overpriced, lost customers, then dropped price. Need sweet spot"
        elif rec['recommendation_type'] == 'NEEDS_FRACTIONAL_ZONES':
            problem = f"âš ï¸ Lowest zone but still losing {rec['lapsed_customers']} customers"
        else:
            problem = f"Zone {rec['current_zone']} higher than peer sites"
        
        # Add GREEN FLAG indicator
        green_flag_status = 'ðŸŸ¢ HIGH' if rec.get('green_flag_rate', 0) >= 0.50 else 'ðŸŸ¡ MEDIUM' if rec.get('green_flag_rate', 0) >= 0.30 else 'ðŸ”´ LOW'
        
        # Get YoY customer data for this combo
        combo = rec['company_combo']
        yoy_data = yoy_by_combo.get(combo, {})
        
        cy_cust = yoy_data.get('distinct_customers_cy', 'N/A')
        py_cust = yoy_data.get('distinct_customers_py', 'N/A')
        change = yoy_data.get('customer_change', 'N/A')
        
        if isinstance(change, (int, float)):
            yoy_status = f"{change:+,} ({change/py_cust*100:+.1f}%)" if py_cust != 'N/A' and py_cust > 0 else f"{change:+,}"
        else:
            yoy_status = 'N/A'
        
        quick_wins.append({
            'Priority': i,
            'OpCo': rec['company_name'],
            'Category': f"{rec['cuisine']} - AG{rec['attribute_group']}",
            'Current_Zone': rec['current_zone'],
            'Move_To_Zone': rec['recommended_zone'],
            'Customers_This_Year': cy_cust if cy_cust != 'N/A' else 'N/A',
            'Customers_Last_Year': py_cust if py_cust != 'N/A' else 'N/A',
            'YoY_Customer_Change': yoy_status,
            'Problem': problem,
            'Expected_Result': rec['expected_result'],
            'Timeline': rec['timeline'],
            'Risk': rec['risk_level'],
            'Volume_at_Stake': f"{rec['total_volume']:,.0f} lbs",
            'Current_Zone_Stickiness': green_flag_status,
            'Consistent_Buyers': rec.get('consistent_buyers', 0)
        })
    
    return pd.DataFrame(quick_wins)

def create_yoy_customer_dashboard(yoy_metrics: Dict) -> pd.DataFrame:
    """
    Year-over-year customer tracking dashboard.
    
    Simple explanation:
    "Shows clearly: Are we gaining or losing customers? Which combos are growing?"
    """
    
    if not yoy_metrics or 'by_combo' not in yoy_metrics:
        return pd.DataFrame([{'Message': 'No YoY data available'}])
    
    combo_df = yoy_metrics['by_combo'].copy()
    
    # Sort by biggest customer losses first (need attention)
    combo_df = combo_df.sort_values('customer_change', ascending=True)
    
    # Add status indicators
    def status(change):
        if change > 10:
            return 'ðŸŸ¢ GROWING'
        elif change > 0:
            return 'ðŸŸ¡ SLIGHT GROWTH'
        elif change >= -10:
            return 'ðŸŸ  SLIGHT DECLINE'
        else:
            return 'ðŸ”´ MAJOR DECLINE'
    
    combo_df['Status'] = combo_df['customer_change'].apply(status)
    
    # Format for display
    display = combo_df[[
        'Company_Combo_Key', 'distinct_customers_cy', 'distinct_customers_py',
        'customer_change', 'customer_change_pct', 'retention_rate',
        'new_customers', 'lost_customers', 'Status'
    ]].copy()
    
    display.columns = [
        'Combo', 'Customers This Year', 'Customers Last Year',
        'Change', 'Change %', 'Retention Rate %',
        'New Customers', 'Lost Customers', 'Status'
    ]
    
    # Format percentages
    display['Change %'] = display['Change %'].apply(lambda x: f"{x:+.1f}%")
    display['Retention Rate %'] = display['Retention Rate %'].apply(lambda x: f"{x:.1f}%")
    
    return display

def create_zone_stickiness_report(consistency_analysis: pd.DataFrame) -> pd.DataFrame:
    """
    Show which zones have the best customer retention (GREEN FLAGS!).
    
    8th Grade Explanation:
    "This shows which zones keep customers coming back week after week. 
    High stickiness = GREEN FLAG! Low stickiness = customers try once and leave."
    """
    
    if consistency_analysis is None or consistency_analysis.empty:
        return pd.DataFrame([{'Message': 'No consistency data available'}])
    
    stickiness = consistency_analysis.copy()
    
    # Add color-coded flags
    def flag(rate):
        if rate >= 0.60:
            return 'ðŸŸ¢ EXCELLENT'
        elif rate >= 0.40:
            return 'ðŸŸ¡ GOOD'
        elif rate >= 0.20:
            return 'ðŸŸ  FAIR'
        else:
            return 'ðŸ”´ POOR'
    
    stickiness['Stickiness_Rating'] = stickiness['green_flag_rate'].apply(flag)
    
    # Sort by best stickiness
    stickiness = stickiness.sort_values('green_flag_rate', ascending=False)
    
    # Format for display
    display = stickiness[[
        'Company_Combo_Key', 'Zone', 'distinct_customers', 
        'consistent_buyers', 'green_flag_rate', 'avg_consistency_rate',
        'Stickiness_Rating'
    ]].copy()
    
    display.columns = [
        'Combo', 'Zone', 'Distinct Customers', 'Consistent Buyers',
        'GREEN FLAG Rate', 'Avg Consistency', 'Stickiness Rating'
    ]
    
    # Format percentages
    display['GREEN FLAG Rate'] = display['GREEN FLAG Rate'].apply(lambda x: f"{x:.1%}")
    display['Avg Consistency'] = display['Avg Consistency'].apply(lambda x: f"{x:.1%}")
    
    return display

def create_customer_recovery_tracker(recommendations: List[Dict]) -> pd.DataFrame:
    """Show recovery opportunity by combo."""
    
    recovery_data = []
    
    for rec in recommendations:
        if rec['lapsed_customers'] > 0:
            recovery_data.append({
                'OpCo': rec['company_name'],
                'Category': f"{rec['cuisine']} - AG{rec['attribute_group']}",
                'Current_Zone': rec['current_zone'],
                'Active_Customers': rec['active_customers'],
                'Lapsed_Customers': rec['lapsed_customers'],
                'Total_Customers': rec['total_customers'],
                'Lapsed_Rate': f"{rec['lapsed_pct']:.1%}",
                'Recovery_Potential': 'ðŸ”¥ HIGH' if rec['lapsed_pct'] >= 0.30 else 'âœ… MEDIUM' if rec['lapsed_pct'] >= 0.15 else 'âšª LOW',
                'Recommended_Action': f"Drop to Zone {rec['recommended_zone']}",
                'Expected_Recovery': f"{int(rec['lapsed_customers'] * 0.6)} customers"
            })
    
    df = pd.DataFrame(recovery_data)
    
    if not df.empty:
        df['_sort'] = df['Lapsed_Customers']
        df = df.sort_values('_sort', ascending=False).drop('_sort', axis=1)
    
    return df


def create_reactive_pricing_alerts(reactive_flags: Dict) -> pd.DataFrame:
    """Flag combos with reactive pricing."""
    
    alerts = []
    
    for combo, flag in reactive_flags.items():
        alerts.append({
            'Combo': combo,
            'What_Happened': flag['stakeholder_message'],
            'High_Zone_Used': flag['from_zone'],
            'Panic_Drop_To': flag['to_zone'],
            'Customers_Lost_First': flag['customers_lapsed_before_drop'],
            'Likely_True_Optimal': flag['likely_true_optimal'],
            'Recovery_Rate': f"{flag['volume_recovery_rate']:+.1%}",
            'Trust_Level': flag['trust_level']
        })
    
    return pd.DataFrame(alerts)


def create_implementation_timeline(recommendations: List[Dict]) -> pd.DataFrame:
    """Week-by-week plan."""
    
    timeline = []
    
    # Week 1-2
    week_1_2 = [r for r in recommendations if '2-4 weeks' in r['timeline']][:3]
    if week_1_2:
        timeline.append({
            'Timeframe': 'Week 1-2',
            'Action': 'Implement high-recovery moves',
            'Combos': len(week_1_2),
            'Expected_Impact': f"+{sum(r['total_volume'] * 0.3 for r in week_1_2):,.0f} lbs",
            'Focus': 'Easy wins - lapsed customer recovery'
        })
    
    # Week 3-4
    timeline.append({
        'Timeframe': 'Week 3-4',
        'Action': 'Monitor Week 1-2 moves',
        'Combos': len(week_1_2),
        'Expected_Impact': 'Confirmation of gains',
        'Focus': 'Track customer reactivation'
    })
    
    # Week 5-8
    week_5_8 = [r for r in recommendations if '4-6 weeks' in r['timeline']][:5]
    if week_5_8:
        timeline.append({
            'Timeframe': 'Week 5-8',
            'Action': 'Implement reactive corrections + peer consensus',
            'Combos': len(week_5_8),
            'Expected_Impact': f"+{sum(r['total_volume'] * 0.15 for r in week_5_8):,.0f} lbs",
            'Focus': 'Finding optimal zones'
        })
    
    return pd.DataFrame(timeline)


def create_learning_tracker_tab(learning_engine: LearningEngine) -> pd.DataFrame:
    """Show what we've learned."""
    
    completed = learning_engine.get_completed_recommendations()
    
    if not completed:
        return pd.DataFrame([{
            'Message': 'No completed recommendations yet',
            'Next_Steps': 'Implement recommendations, check back in 4-6 weeks'
        }])
    
    learning_data = []
    
    for rec in completed:
        learning_data.append({
            'Date_Recommended': rec.date_recommended,
            'Date_Implemented': rec.date_implemented,
            'OpCo': rec.company_name,
            'Category': rec.category_description,
            'Zone_Change': f"{rec.from_zone} â†’ {rec.to_zone}",
            'Type': rec.recommendation_type,
            'Predicted_Volume_Lift': f"{rec.predicted_volume_lift:,.0f} lbs",
            'Actual_Volume_Lift': f"{rec.actual_volume_lift:,.0f} lbs" if rec.actual_volume_lift else 'N/A',
            'Outcome': rec.outcome_vs_prediction or 'Pending',
            'Weeks_to_Result': rec.weeks_to_result or 'N/A',
            'Lessons': '; '.join(rec.lessons_learned) if rec.lessons_learned else 'None yet'
        })
    
    return pd.DataFrame(learning_data)

# ============================================================================
# COLUMN NORMALIZATION
# ============================================================================

def _normalize_columns(df: pd.DataFrame, data_config: DataConfiguration) -> pd.DataFrame:
    """Normalize column names based on config."""
    
    # Basic renames
    rename_map = {
        'Pounds CY': 'Pounds_CY',
        'Pounds PY': 'Pounds_PY',
        'Delta Pounds YoY': 'Delta_YoY_Lbs'
    }
    
    df = df.rename(columns=rename_map)
    
    # Ensure numeric
    numeric_cols = ['Pounds_CY', 'Pounds_PY', 'Delta_YoY_Lbs']
    if data_config.zone_column in df.columns:
        numeric_cols.append(data_config.zone_column)
    
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
    
    # Customer ID as string
    if data_config.customer_id_column in df.columns:
        df[data_config.customer_id_column] = df[data_config.customer_id_column].astype(str)
    
    # Create combo key
    grouping_cols = data_config.get_grouping_columns()
    
    combo_parts = []
    for col in grouping_cols:
        if col in df.columns:
            combo_parts.append(df[col].astype(str))
        else:
            combo_parts.append(pd.Series(['UNK'] * len(df)))
    
    if combo_parts:
        df['Company_Combo_Key'] = combo_parts[0]
        for part in combo_parts[1:]:
            df['Company_Combo_Key'] = df['Company_Combo_Key'] + '_' + part
    
    # Parse zone if needed
    if 'Price Zone ID' in df.columns and data_config.zone_column not in df.columns:
        df[data_config.zone_column] = (
            df['Price Zone ID']
            .astype(str).str.split('-').str[-1]
            .str.extract(r'(\d+)')[0]
        )
        df[data_config.zone_column] = pd.to_numeric(df[data_config.zone_column], errors='coerce').fillna(5)
    
    return df

# ============================================================================
# BULLETPROOF EXCEL FORMATTING (NO STAKEHOLDER ACTION NEEDED)
# ============================================================================

def _format_excel_sheets(writer):
    """
    Format Excel so stakeholders can open and read immediately.
    NO column resizing, NO row adjusting, NO manual work needed.
    
    Simple explanation:
    "Make the Excel file look perfect the moment they open it. 
    They shouldn't have to click anything."
    """
    wb = writer.book
    
    # Define styles
    header_fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')
    header_font = Font(bold=True, color='FFFFFF', size=12)
    
    priority_fill = PatternFill(start_color='FFE699', end_color='FFE699', fill_type='solid')
    alert_fill = PatternFill(start_color='FFC7CE', end_color='FFC7CE', fill_type='solid')
    success_fill = PatternFill(start_color='C6EFCE', end_color='C6EFCE', fill_type='solid')
    medium_fill = PatternFill(start_color='FFF4CC', end_color='FFF4CC', fill_type='solid')
    
    # Border for readability
    thin_border = Border(
        left=Side(style='thin', color='D3D3D3'),
        right=Side(style='thin', color='D3D3D3'),
        top=Side(style='thin', color='D3D3D3'),
        bottom=Side(style='thin', color='D3D3D3')
    )
    
    for sheet_name in wb.sheetnames:
        ws = wb[sheet_name]
        
        # ==========================================
        # STEP 1: Format Headers
        # ==========================================
        for cell in ws[1]:
            cell.fill = header_fill
            cell.font = header_font
            cell.alignment = Alignment(
                horizontal='center', 
                vertical='center', 
                wrap_text=True
            )
            cell.border = thin_border
        
        # ==========================================
        # STEP 2: Auto-fit ALL columns properly
        # ==========================================
        for column_cells in ws.columns:
            column_letter = get_column_letter(column_cells[0].column)
            
            # Calculate max length needed
            max_length = 0
            for cell in column_cells:
                try:
                    if cell.value:
                        # Handle multi-line content
                        lines = str(cell.value).split('\n')
                        max_line_length = max(len(line) for line in lines)
                        max_length = max(max_length, max_line_length)
                except:
                    pass
            
            # Set width with padding
            # Minimum 12 chars, maximum 80 chars for readability
            adjusted_width = min(max(max_length + 3, 12), 80)
            ws.column_dimensions[column_letter].width = adjusted_width
        
        # ==========================================
        # STEP 3: Set row heights and wrap text
        # ==========================================
        for row_idx, row in enumerate(ws.iter_rows(min_row=1), start=1):
            
            # Header row taller
            if row_idx == 1:
                ws.row_dimensions[row_idx].height = 30
            else:
                # Calculate height based on content
                max_lines = 1
                for cell in row:
                    if cell.value:
                        lines = str(cell.value).count('\n') + 1
                        max_lines = max(max_lines, lines)
                
                # 15 points per line + padding
                ws.row_dimensions[row_idx].height = max(15 * max_lines + 5, 20)
            
            # Format all cells in row
            for cell in row:
                if row_idx > 1:  # Not header
                    cell.alignment = Alignment(
                        horizontal='left',
                        vertical='top',
                        wrap_text=True
                    )
                    cell.border = thin_border
        
        # ==========================================
        # STEP 4: Freeze panes (header stays visible)
        # ==========================================
        ws.freeze_panes = 'A2'
        
        # ==========================================
        # STEP 5: Sheet-specific formatting
        # ==========================================
        
        # Executive Summary - Make it POP
        if 'EXECUTIVE_SUMMARY' in sheet_name:
            for row in range(2, ws.max_row + 1):
                for col in range(1, ws.max_column + 1):
                    ws.cell(row, col).font = Font(size=12, bold=True)
        
        # Top 5 Moves - Highlight priorities
        if 'TOP_5_MOVES' in sheet_name:
            priority_col = None
            
            # Find Priority column
            for col in range(1, ws.max_column + 1):
                if ws.cell(1, col).value == 'Priority':
                    priority_col = col
                    break
            
            if priority_col:
                for row in range(2, min(5, ws.max_row + 1)):  # Top 3 priorities
                    for col in range(1, ws.max_column + 1):
                        ws.cell(row, col).fill = priority_fill
                        ws.cell(row, col).font = Font(bold=True, size=11)
        
        # Recovery Opportunities - Color code by potential
        if 'RECOVERY_OPPORTUNITIES' in sheet_name:
            potential_col = None
            
            # Find Recovery_Potential column
            for col in range(1, ws.max_column + 1):
                if 'Recovery_Potential' in str(ws.cell(1, col).value):
                    potential_col = col
                    break
            
            if potential_col:
                for row in range(2, ws.max_row + 1):
                    cell_value = str(ws.cell(row, potential_col).value)
                    
                    if 'HIGH' in cell_value:
                        for col in range(1, ws.max_column + 1):
                            ws.cell(row, col).fill = success_fill
                    elif 'MEDIUM' in cell_value:
                        for col in range(1, ws.max_column + 1):
                            ws.cell(row, col).fill = medium_fill
        
        # Reactive Alerts - Red highlight
        if 'REACTIVE_ALERTS' in sheet_name:
            for row in range(2, ws.max_row + 1):
                for col in range(1, ws.max_column + 1):
                    ws.cell(row, col).fill = alert_fill
        
        # ==========================================
        # STEP 6: Add filters to data rows
        # ==========================================
        if ws.max_row > 1:
            ws.auto_filter.ref = ws.dimensions
        
        # ==========================================
        # STEP 7: Set print settings (if they print)
        # ==========================================
        ws.page_setup.orientation = ws.ORIENTATION_LANDSCAPE
        ws.page_setup.fitToWidth = 1
        ws.page_setup.fitToHeight = 0  # Allow multiple pages vertically
        
        # Print titles (header repeats on each page)
        ws.print_title_rows = '1:1'
        
        # ==========================================
        # STEP 8: Zoom level for readability
        # ==========================================
        ws.sheet_view.zoomScale = 90  # Slightly zoomed out


def write_stakeholder_excel(recommendations: List[Dict], 
                           reactive_flags: Dict,
                           output_path: str,
                           learning_engine: Optional[LearningEngine] = None,
                           optimization_engine: Optional['FoodserviceZoneEngine'] = None):
    """Create Excel with YoY customer tracking."""
    
    from pandas import ExcelWriter
    
    print("\nðŸ“Š Creating stakeholder dashboard...")
    print("   âš™ï¸  Formatting for immediate readability...")
    
    # Get YoY metrics from engine
    yoy_metrics = None
    if optimization_engine and hasattr(optimization_engine, 'yoy_customer_metrics'):
        yoy_metrics = optimization_engine.yoy_customer_metrics
    
    with ExcelWriter(output_path, engine='openpyxl') as writer:
        
        # ==========================================
        # TAB 1: Executive Summary (with YoY!)
        # ==========================================
        print("   ðŸ“„ Tab 1: Executive Summary")
        exec_summary = create_executive_summary(recommendations, yoy_metrics)
        
        # Convert to vertical layout
        exec_rows = [
            {'Metric': 'Total Opportunities Found', 'Value': exec_summary['total_opportunities']},
            {'Metric': 'High Priority Moves (Do This Week)', 'Value': exec_summary['high_priority_moves']},
            {'Metric': 'Easy Wins - Lapsed Customer Recovery', 'Value': exec_summary['easy_wins_lapsed_recovery']},
            {'Metric': 'Customers We Can Win Back', 'Value': exec_summary['customers_to_win_back']},
            {'Metric': 'Total Volume at Stake', 'Value': exec_summary['total_volume_at_stake']},
            {'Metric': '', 'Value': ''},  # Spacer
            {'Metric': 'ðŸ“Š YEAR-OVER-YEAR CUSTOMER METRICS', 'Value': ''},
            {'Metric': 'Distinct Customers This Year', 'Value': exec_summary.get('distinct_customers_this_year', 'N/A')},
            {'Metric': 'Distinct Customers Last Year', 'Value': exec_summary.get('distinct_customers_last_year', 'N/A')},
            {'Metric': 'Customer Change', 'Value': exec_summary.get('customer_change_count', 'N/A')},
            {'Metric': 'Customer Change %', 'Value': exec_summary.get('customer_change_percent', 'N/A')},
            {'Metric': 'Customer Retention Rate', 'Value': exec_summary.get('customer_retention_rate', 'N/A')},
            {'Metric': 'New Customers Gained', 'Value': exec_summary.get('new_customers_gained', 'N/A')},
            {'Metric': 'Customers Lost', 'Value': exec_summary.get('customers_lost', 'N/A')},
            {'Metric': '', 'Value': ''},  # Spacer
            {'Metric': 'Expected Timeframe for Results', 'Value': exec_summary['expected_timeframe']},
            {'Metric': 'First Action to Take', 'Value': exec_summary['first_action']}
        ]
        exec_df = pd.DataFrame(exec_rows)
        exec_df.to_excel(writer, sheet_name='1_EXECUTIVE_SUMMARY', index=False)
        
        # ==========================================
        # TAB 2: Top 5 Quick Wins (with YoY!)
        # ==========================================
        print("   ðŸ“„ Tab 2: Top 5 Quick Wins")
        quick_wins = create_quick_wins_dashboard(recommendations, yoy_metrics)
        if not quick_wins.empty:
            quick_wins.to_excel(writer, sheet_name='2_TOP_5_MOVES', index=False)
        
        # ==========================================
        # TAB 3: Customer Recovery Tracker
        # ==========================================
        print("   ðŸ“„ Tab 3: Recovery Opportunities")
        recovery = create_customer_recovery_tracker(recommendations)
        
        if not recovery.empty:
            recovery.to_excel(writer, sheet_name='3_RECOVERY_OPPORTUNITIES', index=False)
        
        # ==========================================
        # TAB 4: Reactive Pricing Alerts
        # ==========================================
        if reactive_flags:
            print("   ðŸ“„ Tab 4: Reactive Pricing Alerts")
            alerts = create_reactive_pricing_alerts(reactive_flags)
            if not alerts.empty:
                alerts.to_excel(writer, sheet_name='4_REACTIVE_ALERTS', index=False)
        
        # ==========================================
        # TAB 5: Implementation Timeline
        # ==========================================
        print("   ðŸ“„ Tab 5: Implementation Timeline")
        timeline = create_implementation_timeline(recommendations)
        if not timeline.empty:
            timeline.to_excel(writer, sheet_name='5_TIMELINE', index=False)
        
        # ==========================================
        # TAB 6: Learning Tracker (if available)
        # ==========================================
        if learning_engine:
            print("   ðŸ“„ Tab 6: Learning Tracker")
            learning_df = create_learning_tracker_tab(learning_engine)
            if not learning_df.empty:
                learning_df.to_excel(writer, sheet_name='6_LEARNING_TRACKER', index=False)
        
        # ==========================================
        # TAB 7: All Recommendations (detailed backup)
        # ==========================================
        print("   ðŸ“„ Tab 7: All Recommendations (detailed)")
        all_recs = pd.DataFrame(recommendations)
        
        # Select key columns only
        key_cols = [
            'company_name', 'cuisine', 'attribute_group', 'current_zone',
            'recommended_zone', 'recommendation_type', 'stakeholder_message',
            'expected_result', 'timeline', 'risk_level', 'total_volume',
            'lapsed_customers', 'implementation_priority'
        ]
        
        display_cols = [col for col in key_cols if col in all_recs.columns]
        if display_cols:
            all_recs_display = all_recs[display_cols].copy()
            
            # Simplify column names
            all_recs_display.columns = [
                'OpCo', 'Cuisine', 'Attribute Group', 'Current Zone',
                'Recommended Zone', 'Type', 'Explanation',
                'Expected Result', 'Timeline', 'Risk', 'Volume (lbs)',
                'Lapsed Customers', 'Priority Score'
            ]
            
            all_recs_display.to_excel(writer, sheet_name='7_ALL_RECOMMENDATIONS', index=False)
            
        # ==========================================
        # TAB 8: YoY Customer Tracking (NEW!)
        # ==========================================
        if yoy_metrics:
            print("   ðŸ“„ Tab 8: Year-over-Year Customer Tracking")
            yoy_dashboard = create_yoy_customer_dashboard(yoy_metrics)
            if not yoy_dashboard.empty:
                yoy_dashboard.to_excel(writer, sheet_name='8_YOY_CUSTOMERS', index=False)
        
        # ==========================================
        # TAB 9: Zone Stickiness Report
        # ==========================================
        if optimization_engine and hasattr(optimization_engine, 'consistency_analysis'):
            if optimization_engine.consistency_analysis is not None:
                print("   ðŸ“„ Tab 9: Zone Stickiness Report")
                stickiness = create_zone_stickiness_report(optimization_engine.consistency_analysis)
                if not stickiness.empty:
                    stickiness.to_excel(writer, sheet_name='9_ZONE_STICKINESS', index=False)
                
        # ==========================================
        # Apply formatting to ALL sheets
        # ==========================================
        print("   ðŸŽ¨ Applying formatting...")
        _format_excel_sheets(writer)
    
    print(f"   âœ… Dashboard complete and ready to open!")
    print(f"   ðŸ“ Saved to: {output_path}")


def create_comparison_export(recommendations: List[Dict], 
                             output_path: str,
                             learning_engine: Optional[LearningEngine] = None,
                             yoy_metrics: Optional[Dict] = None):
    """Create CSV with YoY customer baseline."""
    
    comparison_data = []
    
    # Get combo-level YoY data
    yoy_by_combo = {}
    if yoy_metrics and 'by_combo' in yoy_metrics:
        for _, row in yoy_metrics['by_combo'].iterrows():
            yoy_by_combo[row['Company_Combo_Key']] = row
    
    for rec in recommendations:
        combo = rec['company_combo']
        yoy_data = yoy_by_combo.get(combo, {})
        
        comparison_data.append({
            'run_date': datetime.now().strftime('%Y-%m-%d'),
            'combo_key': combo,
            'company': rec['company_name'],
            'cuisine': rec.get('cuisine', 'N/A'),
            'attribute_group': rec.get('attribute_group', 'N/A'),
            'current_zone': rec['current_zone'],
            'recommended_zone': rec['recommended_zone'],
            'recommendation_type': rec['recommendation_type'],
            
            # Volume baselines
            'total_volume_baseline': rec['total_volume'],
            'active_customers_baseline': rec['active_customers'],
            'lapsed_customers_baseline': rec['lapsed_customers'],
            
            # YoY customer baselines (NEW!)
            'distinct_customers_cy_baseline': yoy_data.get('distinct_customers_cy', None),
            'distinct_customers_py_baseline': yoy_data.get('distinct_customers_py', None),
            'customer_retention_rate_baseline': yoy_data.get('retention_rate', None),
            
            # Prediction
            'predicted_volume_lift': rec.get('expected_result', 'N/A'),
            'implementation_priority': rec['implementation_priority'],
            
            # For next run
            'was_implemented': False,
            'actual_volume_after': None,
            'actual_customers_after': None,
            'actual_distinct_customers_cy_after': None  # NEW!
        })
    
    comparison_df = pd.DataFrame(comparison_data)
    comparison_df.to_csv(output_path, index=False)
    
    print(f"   ðŸ’¾ Comparison file saved: {output_path}")
    print(f"   ðŸ“ Use this file to track results in your next run")


# ============================================================================
# MAIN EXECUTION
# ============================================================================

def run_foodservice_zone_optimization(
    current_data_path: str,
    historical_data_paths: Optional[List[str]] = None,
    output_name: str = 'zone_optimization',
    data_config: Optional[DataConfiguration] = None,
    enable_learning: bool = True,
    yoy_lookback_weeks: int = 8,  # â† NEW
    filter_company_number: Optional[str] = None,  # â† NEW
    filter_company_region_id: Optional[str] = None  # â† NEW
):
    """
    Main function with configurable lookback and filtering.
    
    Args:
        current_data_path: Path to current week's data CSV
        historical_data_paths: List of paths to historical data CSVs
        output_name: Base name for output files
        data_config: Configuration for which columns to use
        enable_learning: Whether to use learning system
        yoy_lookback_weeks: How many weeks to compare for YoY (default 8)
        filter_company_number: Optional - filter to specific Company Number
        filter_company_region_id: Optional - filter to specific Company Region ID
    """
    
    print("ðŸš€ Starting Foodservice Zone Optimization Analysis")
    print("=" * 60)
    
    # Setup configuration with new parameters
    if data_config is None:
        data_config = DataConfiguration()
    
    input_config = InputConfiguration(
        current_data_path=current_data_path,
        historical_data_paths=historical_data_paths or []
    )
    
    config = FoodserviceConfig(
        data_config, 
        input_config,
        yoy_lookback_weeks=yoy_lookback_weeks,  # â† NEW
        filter_company_number=filter_company_number,    # â† NEW
        filter_company_region_id=filter_company_region_id       # â† NEW
    )
    
    # Show filter settings
    if filter_company_number or filter_company_region_id:
        print("\nðŸ” FILTERING ENABLED:")
        if filter_company_number:
            print(f"   â€¢ Company Number: {filter_company_number}")
        if filter_company_region_id:
            print(f"   â€¢ Company Region ID: {filter_company_region_id}")
    
    # Validate paths
    valid, issues = input_config.validate_paths()
    if not valid:
        print("âŒ Configuration issues:")
        for issue in issues:
            print(f"   â€¢ {issue}")
        return None, None
    
    # Load current data
    print("\nðŸ“‚ Loading current data...")
    current_df = pd.read_csv(current_data_path, low_memory=False)
    print(f"   âœ… Loaded {len(current_df):,} rows")
    
    # Validate columns
    valid, missing = data_config.validate_dataframe(current_df)
    if not valid:
        print("âŒ Missing required columns:")
        for col in missing:
            print(f"   â€¢ {col}")
        return None, None
    
    # Normalize
    current_df = _normalize_columns(current_df, data_config)
    
    # Apply filters (NEW!)
    current_df = apply_filters(current_df, config)
    if current_df.empty:
        print("âŒ No data after filtering!")
        return None, None
    
    # Load historical data
    historical_df = None
    if historical_data_paths:
        print("\nðŸ“‚ Loading historical data...")
        hist_dfs = []
        for path in historical_data_paths:
            if os.path.exists(path):
                df = pd.read_csv(path, low_memory=False)
                df = _normalize_columns(df, data_config)
                hist_dfs.append(df)
                print(f"   âœ… Loaded {os.path.basename(path)}: {len(df):,} rows")
        
        if hist_dfs:
            historical_df = pd.concat(hist_dfs, ignore_index=True)
            print(f"   âœ… Combined historical: {len(historical_df):,} rows")
            
            # Apply filters to historical too (NEW!)
            historical_df = apply_filters(historical_df, config)
            print(f"   âœ… After filters: {len(historical_df):,} rows")
    
    # Initialize learning engine
    learning_engine = None
    if enable_learning:
        print("\nðŸ§  Initializing learning system...")
        learning_engine = LearningEngine(input_config.learning_file_path)
        print(f"   âœ… Loaded {len(learning_engine.recommendations)} past recommendations")
    
    # Initialize optimization engine
    print("\nðŸ”§ Initializing optimization engine...")
    engine = FoodserviceZoneEngine(config, learning_engine)
    
    # Generate recommendations
    print("\nðŸŽ¯ Generating recommendations...")
    recommendations = engine.generate_recommendations(current_df, historical_df)
    print(f"   âœ… Generated {len(recommendations)} recommendations")
    
    # Get reactive flags
    reactive_flags = engine.reactive_flags
    print(f"   âš ï¸  Found {len(reactive_flags)} reactive pricing patterns")
    
    # Save to learning system
    if learning_engine:
        print("\nðŸ’¾ Saving recommendations to learning system...")
        for rec in recommendations:
            predicted_outcomes = {
                'volume_lift': rec['total_volume'] * 0.20,  # Conservative estimate
                'customer_recovery': rec.get('lapsed_customers', 0) * 0.6,
                'timeline_weeks': 6
            }
            learning_engine.save_recommendation(rec, predicted_outcomes)
        print(f"   âœ… Saved {len(recommendations)} recommendations for future tracking")
    
    # Create output paths
    timestamp = config.get_timestamp()
    excel_path = os.path.join(
        input_config.output_directory,
        f"{output_name}_{timestamp}.xlsx"
    )
    comparison_path = os.path.join(
        input_config.output_directory,
        f"{output_name}_comparison_{timestamp}.csv"
    )
    
    # Write Excel (with perfect formatting)
    write_stakeholder_excel(recommendations, reactive_flags, excel_path, learning_engine, engine)
    
    # Create comparison export
    create_comparison_export(recommendations, comparison_path, learning_engine)
    
    # Print summary
    print("\n" + "=" * 60)
    print("âœ… ANALYSIS COMPLETE")
    print("=" * 60)
    
    exec_summary = create_executive_summary(recommendations)
    print(f"\nðŸ“ˆ KEY FINDINGS:")
    print(f"   â€¢ Total Opportunities: {exec_summary['total_opportunities']}")
    print(f"   â€¢ High Priority Moves: {exec_summary['high_priority_moves']}")
    print(f"   â€¢ Easy Wins (Lapsed Recovery): {exec_summary['easy_wins_lapsed_recovery']}")
    print(f"   â€¢ Volume at Stake: {exec_summary['total_volume_at_stake']}")
    print(f"   â€¢ Customers to Win Back: {exec_summary['customers_to_win_back']:,}")
    print(f"\nðŸŽ¯ FIRST ACTION:")
    print(f"   {exec_summary['first_action']}")
    print(f"\nðŸ“… EXPECTED TIMEFRAME: {exec_summary['expected_timeframe']}")
    
    print(f"\nðŸ“ FILES CREATED:")
    print(f"   â€¢ Dashboard: {excel_path}")
    print(f"   â€¢ Comparison: {comparison_path}")
    if learning_engine:
        print(f"   â€¢ Learning State: {learning_engine.learning_file_path}")
    
    print(f"\nðŸ’¡ NEXT STEPS:")
    print(f"   1. Open {os.path.basename(excel_path)} - NO RESIZING NEEDED!")
    print(f"   2. Review Tab 2 (Top 5 Moves) with stakeholders")
    print(f"   3. Implement recommendations")
    print(f"   4. Re-run this analysis in 4-6 weeks to track results")
    
    return recommendations, reactive_flags


# ============================================================================
# USAGE EXAMPLE
# ============================================================================

if __name__ == "__main__":
    
    # ==========================================
    # STEP 1: Configure your data columns
    # ==========================================
    
    data_config = DataConfiguration(
        # Required columns
        company_column='Company Name',
        customer_id_column='Company Customer Number',
        last_invoice_date_column='Last Invoice Date',
        fiscal_week_column='Fiscal Week Number',
        pounds_cy_column='Pounds_CY',
        pounds_py_column='Pounds_PY',
        zone_column='Zone_Suffix_Numeric',
        
        # Toggle grouping columns
        use_cuisine=True,
        cuisine_column='NPD Cuisine Type',
        
        use_attribute_group=True,
        attribute_group_column='Attribute Group ID',
        
        use_business_center=False,  # â† SET TO True IF YOU NEED IT
        business_center_column='Business Center ID',
        
        use_item_group=False,  # â† SET TO True IF YOU NEED IT
        item_group_column='Item Group ID',
        
        use_price_source=True,
        price_source_column='Price Source Type',

        company_number_column='Company Number',  
        company_region_id_column='Company Region ID'     
    )
        
    # ==========================================
    # STEP 2: Specify your historical file paths
    # ==========================================
    
    HISTORICAL_DATA = [
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_1.csv",
        r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing\historical_shrimp_2.csv"
    ]
    
    # ==========================================
    # STEP 3: Auto-extract "current" from historical
    # ==========================================
    
    print("ðŸ” Extracting current baseline from historical data...")
    
    # Load all historical data
    all_hist = []
    for path in HISTORICAL_DATA:
        if os.path.exists(path):
            df = pd.read_csv(path, low_memory=False)
            all_hist.append(df)
            print(f"   âœ… Loaded {os.path.basename(path)}: {len(df):,} rows")
    
    if not all_hist:
        print("âŒ No historical files found!")
        exit(1)
    
    # Combine all historical
    combined = pd.concat(all_hist, ignore_index=True)
    
    # Normalize columns first
    combined = _normalize_columns(combined, data_config)
    
    # Find latest week
    combined['Fiscal Week Number'] = pd.to_numeric(
        combined['Fiscal Week Number'], 
        errors='coerce'
    )
    
    latest_week = combined['Fiscal Week Number'].max()
    print(f"\n   ðŸ“… Latest week in data: {latest_week}")
    print(f"   ðŸ“Š Using this as 'current' baseline")
    
    # Extract current (latest week)
    current_df = combined[combined['Fiscal Week Number'] == latest_week].copy()
    print(f"   âœ… Current baseline: {len(current_df):,} rows")
    
    # Save current to temporary file for the function
    temp_current_path = os.path.join(
        data_config.output_directory if hasattr(data_config, 'output_directory') else r"C:\Users\kmor6669\OneDrive - Sysco Corporation\Desktop\Pricing",
        f"temp_current_{latest_week}.csv"
    )
    current_df.to_csv(temp_current_path, index=False)
    print(f"   ðŸ’¾ Saved current baseline: {temp_current_path}")
    
    # ==========================================
    # STEP 4: Run analysis
    # ==========================================
    
# Generate separate reports for each company
company_numbers = ['13']  # Can add multiple Company Numbers in a region(s)

for company_number in company_numbers:
    print(f"\n{'='*60}")
    print(f"Processing Company Number: {company_number}")
    print('='*60)
    
    # NEW: Look up company name from the data
    filtered_data = combined[
        combined[data_config.company_number_column].astype(str) == str(company_number)
    ]
    
    if filtered_data.empty:
        print(f"   âŒ No data found for Company Number '{company_number}'")
        continue
    
    # Get company name from first row (all rows for this ID have same name)
    company_name = filtered_data[data_config.company_column].iloc[0]
    print(f"   ðŸ“ Company Name: {company_name}")
    
    # Clean company name for filename (remove spaces, special chars)
    safe_company_name = str(company_name).replace(' ', '_').replace('/', '_').replace('&', 'and')
    
    # ==========================================
    # Run analysis for this company
    # ==========================================
    
    recommendations, reactive_flags = run_foodservice_zone_optimization(
        current_data_path=temp_current_path,
        historical_data_paths=HISTORICAL_DATA,
        output_name=f'{safe_company_name}_{company_number}_shrimp_zones',  # â† HAS NAME!
        data_config=data_config,
        enable_learning=True,
        yoy_lookback_weeks=8,
        filter_company_number=company_number,
        filter_company_region_id=None
    )
    
    print(f"\nâœ… Report complete for {company_name} (ID: {company_number})")
